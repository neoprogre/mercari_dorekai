import time
import csv
import os
import re
import random
from typing import Optional

from playwright.sync_api import sync_playwright, TimeoutError, Page

# --- è¨­å®š ---
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL
TARGET_URL = "https://jp.mercari.com/user/profile/175075619"
# å‡ºåŠ›ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«å
OUTPUT_CSV = "mercari_profile_products.csv"
# --- è¨­å®šã“ã“ã¾ã§ ---

def log(message):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹"""
    print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}")

def extract_product_number(name: str) -> Optional[str]:
    """å•†å“åã‹ã‚‰å“ç•ªï¼ˆå…ˆé ­ã®3-5æ¡ã®æ•°å­—ï¼‰ã‚’æŠ½å‡ºã™ã‚‹"""
    if not isinstance(name, str):
        return None
    match = re.match(r'^(\d{3,5})\s', name)
    return match.group(1) if match else None

def get_product_details(page: Page, url: str) -> dict:
    """å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
    log(f"  è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹: {url}")
    page.goto(url, wait_until='domcontentloaded', timeout=60000)
    time.sleep(1)

    product_data = {"URL": url}

    # å•†å“å
    if page.locator('[data-testid="name"]').count() > 0:
        name = page.locator('[data-testid="name"]').text_content().strip()
    else:
        name = page.locator('h1').first.text_content().strip()

    product_data["å“ç•ª"] = extract_product_number(name)
    product_data["å•†å“å"] = name

    # ä¾¡æ ¼
    if page.locator('[data-testid="price"]').count() > 0:
        price_text = page.locator('[data-testid="price"]').text_content()
        product_data["ä¾¡æ ¼"] = re.sub(r'[^\d]', '', price_text)
    else:
        product_data["ä¾¡æ ¼"] = ""

    # ã„ã„ã­æ•°
    if page.locator('[data-testid="icon-heart-button"]').count() > 0:
        product_data["ã„ã„ã­æ•°"] = page.locator('[data-testid="icon-heart-button"]').text_content().strip()
    else:
        product_data["ã„ã„ã­æ•°"] = "0"

    # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    if page.locator('[data-location="item_details:item_info:comment_icon_button"]').count() > 0:
        product_data["ã‚³ãƒ¡ãƒ³ãƒˆæ•°"] = page.locator('[data-location="item_details:item_info:comment_icon_button"]').text_content().strip()
    else:
        product_data["ã‚³ãƒ¡ãƒ³ãƒˆæ•°"] = "0"

    # å•†å“èª¬æ˜
    if page.locator('[data-testid="description"]').count() > 0:
        product_data["å•†å“èª¬æ˜"] = page.locator('[data-testid="description"]').text_content().strip()
    else:
        product_data["å•†å“èª¬æ˜"] = ""

    # ç”»åƒURL (ã‚«ãƒ«ãƒ¼ã‚»ãƒ«ã‹ã‚‰å…¨ç”»åƒå–å¾—)
    images = page.locator('[data-testid="carousel-item"] img').all()
    image_urls = []
    for img in images:
        src = img.get_attribute('src')
        if src:
            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šé™¤
            src_clean = src.split('?')[0]
            image_urls.append(src_clean)
    # é‡è¤‡æ’é™¤ã—ã¦ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ä¿å­˜
    product_data["ç”»åƒURL"] = ",".join(list(dict.fromkeys(image_urls)))

    product_data["å•†å“ID"] = url.split('/item/')[1] if '/item/' in url else ''

    # è©³ç´°æƒ…å ± (data-testid ãƒ™ãƒ¼ã‚¹ã§å–å¾—)
    def get_detail(testid):
        loc = page.locator(f'[data-testid="{testid}"]')
        if loc.count() > 0:
            return loc.text_content().strip()
        return ""

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ (ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆã‹ã‚‰å–å¾—)
    if page.locator('[data-testid="item-detail-category"]').count() > 0:
        product_data["ã‚«ãƒ†ã‚´ãƒªãƒ¼"] = " > ".join(page.locator('[data-testid="item-detail-category"] a').all_text_contents())
    else:
        product_data["ã‚«ãƒ†ã‚´ãƒªãƒ¼"] = ""

    product_data["ã‚µã‚¤ã‚º"] = get_detail("ã‚µã‚¤ã‚º")
    product_data["ãƒ–ãƒ©ãƒ³ãƒ‰"] = get_detail("ãƒ–ãƒ©ãƒ³ãƒ‰")
    product_data["çŠ¶æ…‹"] = get_detail("å•†å“ã®çŠ¶æ…‹")
    product_data["é…é€æ–™ã®è² æ‹…"] = get_detail("é…é€æ–™ã®è² æ‹…")
    product_data["é…é€ã®æ–¹æ³•"] = get_detail("é…é€ã®æ–¹æ³•")
    product_data["ç™ºé€å…ƒã®åœ°åŸŸ"] = get_detail("ç™ºé€å…ƒã®åœ°åŸŸ")
    product_data["ç™ºé€ã¾ã§ã®æ—¥æ•°"] = get_detail("ç™ºé€ã¾ã§ã®æ—¥æ•°")

    return product_data


def main():
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¡ãƒ«ã‚«ãƒªã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰å‡ºå“ä¸­ã®å•†å“æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    """
    log("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    
    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=False) # headless=Trueã«ã™ã‚‹ã¨ãƒ–ãƒ©ã‚¦ã‚¶éè¡¨ç¤º
            page = browser.new_page(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            
            log(f"ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™: {TARGET_URL}")
            page.goto(TARGET_URL, wait_until='domcontentloaded', timeout=90000) # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’90ç§’ã«å»¶é•·
            time.sleep(2) # ãƒšãƒ¼ã‚¸ãŒå®‰å®šã™ã‚‹ã®ã‚’å¾…ã¤

            # ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ãŒãªããªã‚‹ã¾ã§ã‚¯ãƒªãƒƒã‚¯ã—ç¶šã‘ã‚‹
            log("ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’é †æ¬¡ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¨å•†å“ã‚’è¡¨ç¤ºã—ã¾ã™...")
            while True:
                try:
                    item_count_before = page.locator('li[data-testid="item-cell"]').count()

                    # ãƒšãƒ¼ã‚¸æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    log(f"    ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã—ãŸ (ç¾åœ¨ {item_count_before}ä»¶)ã€‚æ–°ã—ã„ãƒœã‚¿ãƒ³/ã‚¢ã‚¤ãƒ†ãƒ ã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¡ã¾ã™...")
                    time.sleep(1.5) # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆãŒå‡¦ç†ã•ã‚Œã‚‹ã®ã‚’å¾…ã¤æ™‚é–“ã‚’å»¶é•·

                    # ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æ¢ã™
                    load_more_button = page.locator('button:has-text("ã‚‚ã£ã¨è¦‹ã‚‹")')
                    
                    # ãƒœã‚¿ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã‚’å¾…ã¤ (å°‘ã—é•·ã‚ã«)
                    load_more_button.wait_for(state="visible", timeout=7000)
                    
                    # ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
                    load_more_button.click()
                    log("    ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã—ãŸã€‚")

                    # ã‚¯ãƒªãƒƒã‚¯å¾Œã€ã‚¢ã‚¤ãƒ†ãƒ æ•°ãŒå¢—ãˆã‚‹ã®ã‚’å¾…ã¤ (ã“ã‚ŒãŒæœ€ã‚‚ç¢ºå®Ÿãªå¾…æ©Ÿæ–¹æ³•)
                    page.wait_for_function(
                        expression=f"document.querySelectorAll('li[data-testid=\"item-cell\"]').length > {item_count_before}",
                        timeout=15000 # 15ç§’å¾…ã£ã¦ã‚‚å¢—ãˆãªã‘ã‚Œã°ã€èª­ã¿è¾¼ã¿å®Œäº†ã‹ã‚¨ãƒ©ãƒ¼ã¨åˆ¤æ–­
                    )
                    
                    item_count_after = page.locator('li[data-testid="item-cell"]').count()
                    log(f"    ã‚¢ã‚¤ãƒ†ãƒ æ•°ãŒå¢—åŠ ã—ã¾ã—ãŸ: {item_count_before} -> {item_count_after}")
                    
                    # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã«å°‘ã—é–“ã‚’ç½®ã
                    time.sleep(1)

                except TimeoutError:
                    log("ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å…¨ä»¶èª­ã¿è¾¼ã¿å®Œäº†ã¨åˆ¤æ–­ã—ã¾ã™ã€‚")
                    break
                except Exception as e:
                    log(f"ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    break

            # ã¾ãšã¯è²©å£²ä¸­ã®å•†å“ã®URLã‚’ã™ã¹ã¦å–å¾—
            all_items = page.locator('li[data-testid="item-cell"]').all()
            log(f"åˆè¨ˆ {len(all_items)} ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ è¦ç´ ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
            
            products_to_scrape = []
            for item in all_items:
                try:
                    item.scroll_into_view_if_needed(timeout=5000)
                    if item.locator('div[data-testid="thumbnail-sticker"]').count() == 0:
                        link_loc = item.locator('a[data-testid="thumbnail-link"]')
                        if link_loc.count() > 0:
                            relative_url = link_loc.get_attribute('href')
                            url = f"https://jp.mercari.com{relative_url}"
                            products_to_scrape.append(url)
                except Exception as e:
                    log(f"URLã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰: {e}")

            # å„å•†å“ãƒšãƒ¼ã‚¸ã‚’å·¡å›ã—ã¦è©³ç´°æƒ…å ±ã‚’å–å¾—
            scraped_data = []
            try:
                for i, url in enumerate(products_to_scrape):
                    log(f"--- å•†å“ {i+1}/{len(products_to_scrape)} ã‚’å‡¦ç†ä¸­ ---")
                    
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            product_details = get_product_details(page, url)
                            scraped_data.append(product_details)
                            time.sleep(random.uniform(3.0, 6.0)) # å¾…æ©Ÿæ™‚é–“ã‚’å°‘ã—å»¶é•·ã—ã¦å®‰å®šåŒ–
                            break # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                        except Exception as e:
                            if attempt < max_retries - 1:
                                wait_time = 10 * (attempt + 1)
                                log(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries}): {e}ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                                time.sleep(wait_time)
                            else:
                                log(f"âŒ è©³ç´°ãƒšãƒ¼ã‚¸ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰: {url} - {e}")
                                scraped_data.append({"URL": url, "å•†å“å": f"SCRAPE_FAILED: {e}"})
            except KeyboardInterrupt:
                log("ğŸ›‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã™ã€‚")

            # CSVã«ä¿å­˜
            if scraped_data:
                log(f"å…¨ {len(scraped_data)} ä»¶ã®å•†å“ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                fieldnames = [
                    'å“ç•ª', 'å•†å“ID', 'å•†å“å', 'ä¾¡æ ¼', 'ã„ã„ã­æ•°', 'ã‚³ãƒ¡ãƒ³ãƒˆæ•°', 'å•†å“èª¬æ˜', 'ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'ãƒ–ãƒ©ãƒ³ãƒ‰', 
                    'ã‚µã‚¤ã‚º', 'çŠ¶æ…‹', 'é…é€æ–™ã®è² æ‹…', 'é…é€ã®æ–¹æ³•', 
                    'ç™ºé€å…ƒã®åœ°åŸŸ', 'ç™ºé€ã¾ã§ã®æ—¥æ•°', 'URL', 'ç”»åƒURL'
                ]
                script_dir = os.path.dirname(os.path.abspath(__file__))
                output_path = os.path.join(script_dir, OUTPUT_CSV)
                with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
                    writer.writeheader()
                    writer.writerows(scraped_data)
                log(f"ãƒ‡ãƒ¼ã‚¿ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            
            browser.close()

        except Exception as e:
            log(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    log("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()