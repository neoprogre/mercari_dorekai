from playwright.sync_api import sync_playwright
from urllib.parse import urlparse
import os, time, requests, json, glob, csv, re

# === è¨­å®š =========================
# SHOP_URL ã¯ã‚‚ã†ä½¿ã„ã¾ã›ã‚“ï¼ˆå€‹åˆ¥å•†å“ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚ï¼‰
IMAGE_DIR = r"\\LS210DNBD82\share\å¹³è‰¯\Python\mercari_dorekai\mercari_images"
# product_data_*.csv ã‚’æ¢ã™ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = r"\\LS210DNBD82\share\å¹³è‰¯\Python\mercari_dorekai\downloads"
PRODUCT_DATA_GLOB = os.path.join(DATA_DIR, "product_data_*.csv")
SCROLL_TIMES = 10  # ä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ï¼ˆå•†å“ãƒšãƒ¼ã‚¸å†…ã®ç”»åƒå–å¾—ã®ãŸã‚ï¼‰
WAIT_BETWEEN_IMAGES = 0.3  # ç§’
# =================================

os.makedirs(IMAGE_DIR, exist_ok=True)

def get_extension_from_content_type(content_type):
    """Content-Type æ–‡å­—åˆ—ã‹ã‚‰æ‹¡å¼µå­ã‚’æ¨æ¸¬ã™ã‚‹"""
    mapping = {"image/jpeg": ".jpg", "image/png": ".png", "image/gif": ".gif", "image/webp": ".webp"}
    return mapping.get(content_type.split(";")[0].strip().lower(), ".jpg")

def safe_filename(url):
    return os.path.basename(urlparse(url).path)

def download_image(url, path):
    if os.path.exists(path):
        print(f"  â© æ—¢ã«ä¿å­˜æ¸ˆã¿: {path}")
        return
    try:
        # ã¾ãš HEAD ã§ Content-Type ã‚’ç¢ºèªï¼ˆãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚„éç”»åƒã‚’é™¤å¤–ï¼‰
        try:
            head = requests.head(url, allow_redirects=True, timeout=5)
        except Exception:
            head = None

        if head:
            if head.status_code != 200:
                print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚­ãƒƒãƒ—: HEAD returned {head.status_code} - {url}")
                return
            ctype = head.headers.get("Content-Type", "")
            if not ctype.startswith("image/"):
                print(f"  â© éç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {ctype} - {url}")
                return
            # æ‹¡å¼µå­ãŒãªã‘ã‚Œã° Content-Type ã‹ã‚‰æ±ºå®š
            if not os.path.splitext(path)[1]:
                ext = get_extension_from_content_type(ctype)
                path = path + ext if not path.endswith(ext) else path

        # å®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGETï¼‰
        r = requests.get(url, stream=True, timeout=15)
        if r.status_code == 200 and r.headers.get("Content-Type", "").startswith("image/"):
            # æ‹¡å¼µå­ãŒã¾ã ä»˜ã„ã¦ã„ãªã‘ã‚Œã° Content-Type ã‹ã‚‰å†æ±ºå®š
            if not os.path.splitext(path)[1]:
                ctype = r.headers.get("Content-Type", "")
                ext = get_extension_from_content_type(ctype)
                path += ext
            with open(path, "wb") as f:
                for chunk in r.iter_content(1024):
                    if chunk:
                        f.write(chunk)
            print(f"  âœ… ä¿å­˜å®Œäº†: {path}")
            time.sleep(WAIT_BETWEEN_IMAGES)
        else:
            status = r.status_code if r is not None else "N/A"
            ctype = r.headers.get("Content-Type", "") if r is not None else ""
            print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: HTTP {status} / {ctype} - {url}")
    except Exception as e:
        print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e} - {url}")

def find_latest_product_csv():
    files = glob.glob(PRODUCT_DATA_GLOB)
    if not files:
        return None
    latest = max(files, key=os.path.getmtime)
    return latest

def read_product_ids_from_csv(csv_path):
    """
    CSV ã‹ã‚‰ product_id ã‚’èª­ã¿ã€å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ãŒ '2' ã®è¡Œã®ã¿è¿”ã™ã€‚
    - ãƒ˜ãƒƒãƒ€åã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã¦æ¤œå‡ºçŠ¶æ³ã‚’ç¢ºèª
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å€¤ã¯å…¨è§’æ•°å­—ã‚„å°æ•°è¡¨è¨˜ç­‰ã‚’æ­£è¦åŒ–ã—ã¦åˆ¤å®š
    - è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
    """
    product_ids = []
    encodings_to_try = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis', 'euc_jp', 'iso2022_jp', 'latin-1']
    last_exc = None

    def normalize_digits(s):
        if s is None:
            return ""
        s = str(s).strip().strip('"').strip("'")
        trans = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
        return s.translate(trans)

    for enc in encodings_to_try:
        try:
            with open(csv_path, newline='', encoding=enc) as f:
                reader = csv.DictReader(f)
                fieldnames = reader.fieldnames

                print(f"[debug] try encoding={enc}, detected fieldnames={fieldnames}")

                def guess_id_field(fns):
                    for fn in fns:
                        if re.search(r'product.*id|product_id|å•†å“.*id|å•†å“id|^id$', fn or '', re.I):
                            return fn
                    for fn in fns:
                        if re.search(r'\bid\b', fn or '', re.I):
                            return fn
                    return fns[0] if fns else None

                # ã“ã“ã‚’ä¿®æ­£ï¼šã¾ãšã€Œå•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€ã‚’å„ªå…ˆã§æ¢ã™ï¼ˆä»¥å‰ã¯ã€ŒçŠ¶æ…‹ã€ã‚’å…ˆã«å–ã£ã¦ã—ã¾ã£ã¦ã„ãŸï¼‰
                def guess_status_field(fns):
                    for fn in fns:
                        if re.search(r'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|product.*status|product_status|^status$', fn or '', re.I):
                            return fn
                    for fn in fns:
                        if re.search(r'å•†å“.*ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|status', fn or '', re.I):
                            return fn
                    for fn in fns:
                        if re.search(r'çŠ¶æ…‹', fn or '', re.I):
                            return fn
                    return None

                status_counts = {}

                if not fieldnames:
                    f.seek(0)
                    rows = [r for r in csv.reader(f) if any(cell.strip() for cell in r)]
                    if not rows:
                        product_ids = []
                        break
                    header = rows[0]
                    data_rows = rows[1:]
                    id_idx = None
                    status_idx = None
                    for i, hn in enumerate(header):
                        if re.search(r'product.*id|product_id|å•†å“.*id|å•†å“id|^id$', hn or '', re.I):
                            id_idx = i
                            break
                    if id_idx is None:
                        for i, hn in enumerate(header):
                            if re.search(r'\bid\b', hn or '', re.I):
                                id_idx = i
                                break
                    if id_idx is None:
                        id_idx = 0

                    for i, hn in enumerate(header):
                        if re.search(r'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|product.*status|status|çŠ¶æ…‹', hn or '', re.I):
                            status_idx = i
                            break

                    for row in data_rows:
                        if len(row) <= id_idx:
                            continue
                        pid = row[id_idx].strip()
                        if not pid:
                            continue
                        status_val = None
                        if status_idx is not None and len(row) > status_idx:
                            status_val = normalize_digits(row[status_idx])
                        key = status_val or "(empty)"
                        status_counts[key] = status_counts.get(key, 0) + 1
                        m = re.search(r'(\d+)', status_val or "")
                        if m and int(m.group(1)) == 2:
                            product_ids.append(pid)

                else:
                    id_field = guess_id_field(fieldnames)
                    status_field = guess_status_field(fieldnames)

                    print(f"[debug] guessed id_field={id_field}, status_field={status_field}")

                    if not id_field:
                        id_field = fieldnames[0]

                    if not status_field:
                        print("CSV ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†å¯¾è±¡ãªã—ã¨ã—ã¾ã™ã€‚")
                        return []

                    for row in reader:
                        pid = (row.get(id_field) or "").strip()
                        if not pid:
                            continue
                        raw_status = row.get(status_field)
                        status_val = normalize_digits(raw_status)
                        key = status_val or "(empty)"
                        status_counts[key] = status_counts.get(key, 0) + 1
                        m = re.search(r'(\d+)', status_val)
                        if m:
                            try:
                                if int(m.group(1)) == 2:
                                    product_ids.append(pid)
                            except Exception:
                                pass

                print(f"[debug] encoding={enc} status_counts (sample up to 20 keys) = {dict(list(status_counts.items())[:20])}")
                if product_ids:
                    print(f"CSV èª­ã¿è¾¼ã¿æˆåŠŸ: encoding={enc}, å¯¾è±¡ product_id æ•°={len(product_ids)}")
                else:
                    print(f"CSV èª­ã¿è¾¼ã¿: encoding={enc} ã§æˆåŠŸã—ãŸãŒå¯¾è±¡ product_id ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return list(dict.fromkeys(product_ids))

        except Exception as e:
            last_exc = e
            continue

    print(f"CSV èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {last_exc}")
    return []

def get_downloaded_product_ids():
    """IMAGE_DIR å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® product_id ã®ã‚»ãƒƒãƒˆã‚’è¿”ã™"""
    downloaded_ids = set()
    try:
        for filename in os.listdir(IMAGE_DIR):
            # ãƒ•ã‚¡ã‚¤ãƒ«åãŒ "product_id-*" ã®å½¢å¼ã§ã‚ã‚‹ã¨ä»®å®š
            match = re.match(r'^(.*?)-', filename)
            if match:
                downloaded_ids.add(match.group(1))
    except FileNotFoundError:
        print(f"Warning: Image directory not found: {IMAGE_DIR}")
    return downloaded_ids

def process_product_pages(product_ids, downloaded_ids):
    if not product_ids:
        print("å‡¦ç†å¯¾è±¡ã® product_id ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        for product_id in product_ids:
            if not product_id:
                continue
            if product_id in downloaded_ids:
                print(f"\nâ–¶ {product_id} ã¯æ—¢ã«ç”»åƒä¿å­˜æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            product_url = f"https://jp.mercari.com/shops/product/{product_id}"
            print(f"\nâ–¶ {product_id} ã‚’å‡¦ç†ä¸­... ({product_url})")

            try:
                detail_page = browser.new_page()
                detail_page.goto(product_url, timeout=60000)
                # å¿…è¦ãªã‚‰ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆç”»åƒãŒé…å»¶èª­ã¿è¾¼ã¿ã•ã‚Œã‚‹å ´åˆã«å‚™ãˆï¼‰
                prev_height = 0
                for _ in range(SCROLL_TIMES):
                    detail_page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    detail_page.wait_for_timeout(1000)
                    height = detail_page.evaluate("document.body.scrollHeight")
                    if height == prev_height:
                        break
                    prev_height = height

                # ç”»åƒURLæŠ½å‡º
                img_elements = detail_page.query_selector_all('div[data-testid="carousel-item"] picture img')
                if not img_elements:
                    # åˆ¥ã‚»ãƒ¬ã‚¯ã‚¿ã®å¯èƒ½æ€§ã«ã‚‚å¯¾å¿œ
                    img_elements = detail_page.query_selector_all('img')
                if not img_elements:
                    print("  âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    print(f"  ğŸ–¼ï¸ {len(img_elements)}æšã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                    for i, img_element in enumerate(img_elements):
                        img_url = img_element.get_attribute('src')
                        if img_url:
                            # æ‹¡å¼µå­ã¯ .jpg å›ºå®šã«ã›ãšå…ƒURLã‹ã‚‰å–å¾—ã™ã‚‹
                            fname = safe_filename(img_url)
                            # URLã«'@'ãŒå«ã¾ã‚Œã‚‹å ´åˆã‚’è€ƒæ…®ã—ã¦ã€'@'ä»¥é™ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—
                            fname_main = fname.split('@')[0]
                            ext = os.path.splitext(fname_main)[1] or ".jpg"
                            file_name = f"{product_id}-{i+1}{ext}"
                            file_path = os.path.join(IMAGE_DIR, file_name)
                            download_image(img_url, file_path)
                        else:
                            print("  âŒ ç”»åƒURLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                detail_page.close()
            except Exception as e:
                print(f"  âŒ ãƒšãƒ¼ã‚¸å‡¦ç†å¤±æ•— ({product_id}): {e}")

        browser.close()

def main():
    latest_csv = find_latest_product_csv()
    if not latest_csv:
        print(f"product_data_*.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {PRODUCT_DATA_GLOB}")
        return
    print(f"æœ€æ–°ã® CSV: {latest_csv}")
    product_ids = read_product_ids_from_csv(latest_csv)
    print(f"CSV ã‹ã‚‰å–å¾—ã—ãŸå‡¦ç†å¯¾è±¡ã® product_id æ•°: {len(product_ids)}")
    downloaded_ids = get_downloaded_product_ids()
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® product_id æ•°: {len(downloaded_ids)}")
    process_product_pages(product_ids, downloaded_ids)
    print("\nâœ… å‡¦ç†å®Œäº†")

if __name__ == "__main__":
    main()
