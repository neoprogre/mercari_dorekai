import os
import glob
import json
import sys
import threading
import argparse
import logging
import pandas as pd
import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# ---- ãƒ­ã‚°è¨­å®š ----
logger = logging.getLogger("dorekai_scraper")

# ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from yahooku_scraper import scrape_url, save_to_csv, SELLING_URL, CLOSED_URL
    from yahooku_dorekai import setup_driver
    YAHOOKU_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
    YAHOOKU_AVAILABLE = False

def configure_logging(verbose=False):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S",
    )

def load_brand_name_list_from_master(file_path="brand_master_sjis.csv"):
    """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€ï¼ˆå•†å“åã‹ã‚‰ã®æŠ½å‡ºç”¨ï¼‰"""
    brands = []
    try:
        df = pd.read_csv(file_path, encoding='cp932', header=None, usecols=[1])
        brands = df[1].dropna().astype(str).tolist()
        brands.sort(key=len, reverse=True)
        print(f"ğŸ“š ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ {len(brands)} ä»¶ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except FileNotFoundError:
        print(f"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚å•†å“åã‹ã‚‰ã®ãƒ–ãƒ©ãƒ³ãƒ‰æŠ½å‡ºã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
    return brands

def load_brand_master_map(file_path="brand_master_sjis.csv"):
    """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {ãƒ–ãƒ©ãƒ³ãƒ‰ID: {å„åç§°}} ã®è¾æ›¸ã‚’ä½œæˆã™ã‚‹"""
    brand_map = {}
    try:
        # Shift_JISã§èª­ã¿è¾¼ã¿ã€ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã„ã“ã¨ã‚’æƒ³å®š
        df = pd.read_csv(file_path, encoding='cp932', header=None, dtype=str)
        # åˆ—åã‚’å®šç¾©
        df.columns = ['ãƒ–ãƒ©ãƒ³ãƒ‰ID', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰']
        df.dropna(subset=['ãƒ–ãƒ©ãƒ³ãƒ‰ID'], inplace=True)
        # ãƒ–ãƒ©ãƒ³ãƒ‰IDã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã—ã¦è¾æ›¸åŒ–
        brand_map = df.set_index('ãƒ–ãƒ©ãƒ³ãƒ‰ID').to_dict('index')
        print(f"ğŸ“š ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼è¾æ›¸ã‚’ {len(brand_map)} ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except FileNotFoundError:
        print(f"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚IDã‹ã‚‰ã®ãƒ–ãƒ©ãƒ³ãƒ‰åè§£æ±ºã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
    except Exception as e:
        print(f"âŒ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    return brand_map

def extract_product_number(name):
    """å•†å“åã‹ã‚‰å“ç•ªï¼ˆå…ˆé ­ã®3-5æ¡ã®æ•°å­—ï¼‰ã‚’æŠ½å‡ºã™ã‚‹"""
    if not isinstance(name, str):
        return None
    # å…ˆé ­ã®æ•°å­—ï¼ˆç©ºç™½ãªã—ã§ã‚‚OKï¼‰ã‚’å„ªå…ˆ
    match = re.match(r'^\s*(\d{3,5})', name)
    if match:
        return match.group(1)
    # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…ˆé ­ä»˜è¿‘ã®3-5æ¡ã‚’æ¢ã™
    match = re.search(r'(\d{3,5})', name)
    return match.group(1) if match else None

def clean_rakuma_title(name):
    """ãƒ©ã‚¯ãƒã®ã‚¿ã‚¤ãƒˆãƒ«æœ«å°¾ã‚’é™¤å»ã—ã¦å•†å“åã ã‘ã«ã™ã‚‹"""
    if not isinstance(name, str):
        return name
    suffixes = [" | ãƒ•ãƒªãƒã‚¢ãƒ—ãƒª ãƒ©ã‚¯ãƒ", "ï½œãƒ•ãƒªãƒã‚¢ãƒ—ãƒª ãƒ©ã‚¯ãƒ"]
    for suffix in suffixes:
        if suffix in name:
            name = name.split(suffix)[0]
            break
    return name.strip()

def add_duplicate_column(df, subset_col='å“ç•ª'):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«é‡è¤‡ãƒã‚§ãƒƒã‚¯åˆ—ã‚’è¿½åŠ ã™ã‚‹ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    if not df.empty and subset_col in df.columns:
        # keep=Falseã¯é‡è¤‡ã™ã‚‹ã™ã¹ã¦ã®è¡Œã‚’Trueã«ã™ã‚‹
        duplicates = df.duplicated(subset=[subset_col], keep=False) & df[subset_col].notna()
        duplicate_col = duplicates.map({True: 'é‡è¤‡', False: ''})
    else:
        duplicate_col = ['' for _ in range(len(df))]
    
    # pd.concat()ã§åŠ¹ç‡çš„ã«åˆ—ã‚’è¿½åŠ 
    df = pd.concat([df, pd.DataFrame({'é‡è¤‡': duplicate_col}, index=df.index)], axis=1)
    return df

def build_requests_session(headers):
    """ãƒªãƒˆãƒ©ã‚¤ä»˜ãã®HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"],
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update(headers)
    return session

def is_logged_in_rakuma(page):
    """ãƒ©ã‚¯ãƒã§ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    try:
        from bs4 import BeautifulSoup
        html = page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãŒå­˜åœ¨
        # ãƒ­ã‚°ã‚¤ãƒ³æœªæ¸ˆã¿ã®å ´åˆã€ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒå­˜åœ¨
        login_button = soup.find('a', {'href': re.compile(r'login|signin', re.IGNORECASE)})
        is_logged_in = login_button is None
        
        logger.info(f"  [ãƒ©ã‚¯ãƒãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹åˆ¤å®š] ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿: {is_logged_in}")
        return is_logged_in
    except Exception as e:
        logger.warning(f"  ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False

def is_logged_in_mercari_shops(page):
    """ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ã§ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    try:
        from bs4 import BeautifulSoup
        html = page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®å ´åˆã€å•†å“è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹
        # ãƒ­ã‚°ã‚¤ãƒ³æœªæ¸ˆã¿ã®å ´åˆã€ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã‚‹
        login_btn = soup.find('button', {'data-testid': 'login-with-mercari-account'})
        product_items = soup.find_all('li', {'data-testid': 'product'})
        is_logged_in = login_btn is None and len(product_items) > 0
        
        logger.info(f"  [ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹åˆ¤å®š] ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿: {is_logged_in}, å•†å“æ•°: {len(product_items)}")
        return is_logged_in
    except Exception as e:
        logger.warning(f"  ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False

def wait_for_manual_login(page, site_name, timeout_seconds=30, is_logged_in_func=None):
    """
    ãƒ–ãƒ©ã‚¦ã‚¶ã§æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ã‚’è¡Œã†æ™‚é–“ã‚’ç¢ºä¿ã™ã‚‹ï¼ˆãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ãªã‚‰è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    
    Args:
        page: Playwrite ã®ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        site_name: ã‚µã‚¤ãƒˆåï¼ˆãƒ­ã‚°å‡ºåŠ›ç”¨ï¼‰
        timeout_seconds: å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        is_logged_in_func: ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèª
    if is_logged_in_func:
        if is_logged_in_func(page):
            logger.info(f"âœ… {site_name} ã¯æ—¢ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã§ã™ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™\n")
            return
    
    logger.info(f"\nâ³ {site_name} ã«æ‰‹å‹•ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
    logger.info(f"   ãƒ–ãƒ©ã‚¦ã‚¶ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ãƒ­ã‚°ã‚¤ãƒ³ã‚’å®Œäº†ã—ã¦ãã ã•ã„")
    logger.info(f"   {timeout_seconds}ç§’é–“ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å¾…æ©Ÿã—ã¾ã™...")
    logger.info(f"   æº–å‚™ãŒã§ããŸã‚‰ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ [ENTER] ã‚­ãƒ¼ã‚’æŠ¼ã™ã‹ã€ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³çµ‚äº†å¾Œã«ç¶šè¡Œã—ã¾ã™\n")
    
    user_pressed_enter = False
    
    def wait_for_input():
        nonlocal user_pressed_enter
        try:
            input()  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒENTERã‚’æŠ¼ã™ã¾ã§ãƒ–ãƒ­ãƒƒã‚¯
            user_pressed_enter = True
        except:
            pass
    
    # inputã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
    input_thread = threading.Thread(target=wait_for_input, daemon=True)
    input_thread.start()
    
    # ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ã‚’è¡¨ç¤ºï¼ˆ5ç§’ã”ã¨ã€ã¾ãŸã¯æœ€å¾Œã®1ç§’ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼‰
    for remaining in range(timeout_seconds, 0, -1):
        if user_pressed_enter:
            logger.info(f"âœ… ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ [ENTER] ã‚’æŠ¼ã—ã¾ã—ãŸã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™\n")
            return
        # 5ç§’ã”ã¨ã€ã¾ãŸã¯æœ€å¾Œã®1ç§’ã®ã¿ãƒ­ã‚°å‡ºåŠ›
        if remaining % 5 == 0 or remaining == 1:
            logger.info(f"   ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³: {remaining}ç§’...")
        time.sleep(1)
    
    if not user_pressed_enter:
        logger.info(f"âœ… ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³çµ‚äº†ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™\n")

def scrape_rakuma_selling_stats():
    """ãƒ©ã‚¯ãƒã®å‡ºå“ä¸­å•†å“ã‹ã‚‰watchï¼ˆã„ã„ã­ï¼‰ã¨accessï¼ˆé–²è¦§æ•°ï¼‰ã‚’å–å¾—ã™ã‚‹"""
    stats_dict = {}  # {URL: {'watch': 0, 'access': 0}}
    try:
        # user_data_dirã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´
        script_dir = os.path.dirname(os.path.abspath(__file__))
        user_data_dir = os.path.join(script_dir, 'rakuma_user_data')
        
        with sync_playwright() as p:
            logger.info("å‡ºå“ä¸­å•†å“ã®watch/accessæƒ…å ±ã‚’å–å¾—ä¸­...")
            browser = p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,  # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ãŸã‚
                timeout=60000  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’60ç§’ã«è¨­å®š
            )
            page = browser.pages[0] if browser.pages else browser.new_page()
            
            # å‡ºå“ä¸­å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ç§»å‹•ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
            max_attempts = 3
            page_loaded = False
            for attempt in range(max_attempts):
                try:
                    page.goto("https://fril.jp/sell", timeout=30000, wait_until='load')
                    page.wait_for_load_state('networkidle', timeout=10000)
                    page_loaded = True
                    logger.info("âœ… ãƒ©ã‚¯ãƒãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†")
                    break
                except Exception as e:
                    logger.warning(f"  ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿è©¦è¡Œ {attempt+1}/{max_attempts} å¤±æ•—: {e}")
                    if attempt < max_attempts - 1:
                        time.sleep(2)
            
            if not page_loaded:
                logger.warning("  ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™")
            
            time.sleep(1)
            
            # æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ç”¨ã®å¾…æ©Ÿï¼ˆãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾Œã€60ç§’ï¼‰
            wait_for_manual_login(page, "ãƒ©ã‚¯ãƒï¼ˆå‡ºå“ä¸­å•†å“ï¼‰", timeout_seconds=60, is_logged_in_func=is_logged_in_rakuma)
            
            # ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªï¼ˆè¤‡æ•°å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
            max_login_retries = 3
            for retry in range(max_login_retries):
                time.sleep(2)
                if is_logged_in_rakuma(page):
                    logger.info(f"  âœ… ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªå®Œäº†ï¼ˆãƒªãƒˆãƒ©ã‚¤ {retry+1}/{max_login_retries}ï¼‰")
                    break
                else:
                    logger.warning(f"  âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹æœªç¢ºèªï¼ˆãƒªãƒˆãƒ©ã‚¤ {retry+1}/{max_login_retries}ï¼‰")
                    if retry == max_login_retries - 1:
                        logger.error("  âŒ ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªå¤±æ•—ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä¸­æ­¢ã—ã¾ã™")
                        browser.close()
                        return stats_dict
            
            # ãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿ï¼ˆãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã§æœ€æ–°ã®HTMLã‚’å–å¾—ï¼‰
            try:
                page.reload(timeout=30000)
                page.wait_for_load_state('networkidle', timeout=10000)
                logger.info("  âœ… ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸï¼ˆãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªï¼‰")
            except Exception as e:
                logger.warning(f"  âš ï¸ ãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿å¤±æ•—ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰: {e}")
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼šAJAX ãƒ™ãƒ¼ã‚¹ã®ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿
            from bs4 import BeautifulSoup
            max_page = 1
            
            # åˆæœŸãƒšãƒ¼ã‚¸ã‹ã‚‰ç·ãƒšãƒ¼ã‚¸æ•°ã‚’ç‰¹å®š
            try:
                initial_html = page.content()
                initial_soup = BeautifulSoup(initial_html, 'html.parser')
                # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆä¾‹: /ajax/item/selling?page=32ï¼‰
                last_page_link = initial_soup.find('span', class_='last')
                if last_page_link:
                    last_link = last_page_link.find('a')
                    if last_link and 'href' in last_link.attrs:
                        href = last_link['href']
                        page_match = re.search(r'page=(\d+)', href)
                        if page_match:
                            max_page = int(page_match.group(1))
                            logger.info(f"  ğŸ“„ ç·ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—: {max_page}ãƒšãƒ¼ã‚¸")
            except Exception as e:
                logger.warning(f"  âš ï¸ ç·ãƒšãƒ¼ã‚¸æ•°å–å¾—å¤±æ•—: {e}ï¼ˆ1ãƒšãƒ¼ã‚¸ã®ã¿å‡¦ç†ã—ã¾ã™ï¼‰")
                max_page = 1
            
            # ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’è¨ªå•ã—ã¦å•†å“ã‚’åé›†ï¼ˆAJAX ãƒ™ãƒ¼ã‚¹ï¼‰
            logger.info(f"  ğŸ”„ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†é–‹å§‹ï¼ˆ{max_page}ãƒšãƒ¼ã‚¸ï¼‰")
            all_items_html = []
            
            for page_num in range(1, max_page + 1):
                try:
                    if page_num > 1:
                        # ãƒšãƒ¼ã‚¸2ä»¥é™ï¼šãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆAJAX ãƒ­ãƒ¼ãƒ‰ï¼‰
                        # href ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨ï¼ˆè¤‡æ•°ãƒãƒƒãƒé˜²æ­¢ï¼‰
                        page_link = page.locator(f'a[href="/ajax/item/selling?page={page_num}"]')
                        if page_link.count() > 0:
                            try:
                                page_link.click()
                                # networkidle ã¯ãƒ©ã‚¯ãƒã§ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã‚„ã™ã„ãŸã‚ã€domcontentloaded ã«çŸ­ç¸®
                                try:
                                    page.wait_for_load_state('domcontentloaded', timeout=10000)
                                except Exception:
                                    # DOMContentLoaded ã™ã‚‰å¾…ãŸãšã€å˜ã« sleep ã§å¯¾å¿œ
                                    pass
                                time.sleep(1)  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†å®Œäº†ã‚’å¾…ã¤
                                logger.info(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num} ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ­ãƒ¼ãƒ‰ä¸­...")
                            except Exception as click_err:
                                logger.warning(f"  âš ï¸ ãƒšãƒ¼ã‚¸ {page_num} ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {click_err}")
                                continue
                        else:
                            logger.warning(f"  âš ï¸ ãƒšãƒ¼ã‚¸ {page_num} ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            continue
                    
                    # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
                    page_html = page.content()
                    all_items_html.append(page_html)
                    
                    # ãƒšãƒ¼ã‚¸å†…ã®å•†å“æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    page_soup = BeautifulSoup(page_html, 'html.parser')
                    page_items = page_soup.find_all('div', class_='deal-item')
                    logger.info(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num}: {len(page_items)}ä»¶ã®å•†å“ã‚’å–å¾—")
                    
                except Exception as e:
                    logger.warning(f"  âš ï¸ ãƒšãƒ¼ã‚¸ {page_num} ã®å–å¾—å¤±æ•—: {e}")
                    continue
            
            browser.close()
            
            # çµ±åˆHTMLã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜ï¼ˆç¬¬1ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
            if all_items_html:
                debug_html_path = "rakuma_sell_debug.html"
                with open(debug_html_path, "w", encoding="utf-8") as f:
                    f.write(all_items_html[0])
                logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°ç”¨HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {debug_html_path}")
            
            # ã™ã¹ã¦ã®HTMLã‹ã‚‰å•†å“ã‚’æŠ½å‡º
            total_items = 0
            for html_content in all_items_html:
                soup = BeautifulSoup(html_content, 'html.parser')
                items = soup.find_all('div', class_='deal-item')
                total_items += len(items)
                
                for idx, item in enumerate(items):
                    try:
                        # å•†å“URL
                        link_tag = item.find('a', class_='deal-item__info')
                        if not link_tag or 'href' not in link_tag.attrs:
                            continue
                        item_url = link_tag['href']
                        if not item_url.startswith('http'):
                            item_url = urljoin('https://item.fril.jp', item_url)
                        
                        # ã„ã„ã­æ•°ï¼ˆwatchï¼‰
                        watch = 0
                        watch_tag = item.find('span', {'data-test': 'item_like_count'})
                        if watch_tag:
                            watch_text = watch_tag.get_text(strip=True)
                            watch_match = re.search(r'(\d+)', watch_text)
                            if watch_match:
                                watch = int(watch_match.group(1))
                        
                        # é–²è¦§æ•°ï¼ˆaccessï¼‰
                        access = 0
                        access_tag = item.find('span', {'data-test': 'item_view_count'})
                        if access_tag:
                            access_text = access_tag.get_text(strip=True)
                            access_match = re.search(r'(\d+)', access_text)
                            if access_match:
                                access = int(access_match.group(1))
                        
                        stats_dict[item_url] = {'watch': watch, 'access': access}
                        
                        # æœ€åˆã®3ä»¶ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                        if idx < 3 and total_items <= 3:
                            logger.info(f"  [ãƒ‡ãƒãƒƒã‚°] å•†å“ {idx+1}: URL={item_url[:50]}..., watch={watch}, access={access}")
                        
                    except Exception as e:
                        logger.warning(f"  å•†å“ã®watch/accesså–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            logger.info(f"âœ… {total_items} ä»¶ã®å•†å“ã®watch/accessæƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")

            
    except Exception as e:
        logger.error(f"ãƒ©ã‚¯ãƒã®å‡ºå“ä¸­å•†å“æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        # ãƒ–ãƒ©ã‚¦ã‚¶ãŒç¢ºå®Ÿã«é–‰ã˜ã‚‹ã‚ˆã†ã«å¾…æ©Ÿ
        time.sleep(2)
    
    return stats_dict

def scrape_rakuma_draft_items():
    """ãƒ©ã‚¯ãƒã®ä¸‹æ›¸ãã‚¿ãƒ–ï¼ˆå‡ºå“ã—ã¦ã„ãŸï¼‰ã‹ã‚‰å•†å“URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    draft_urls = []
    try:
        # user_data_dirã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´
        script_dir = os.path.dirname(os.path.abspath(__file__))
        user_data_dir = os.path.join(script_dir, 'rakuma_user_data')
        
        with sync_playwright() as p:
            logger.info("ä¸‹æ›¸ãã‚¿ãƒ–ã®å•†å“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰...")
            browser = p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,  # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ãŸã‚
                timeout=60000  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’60ç§’ã«è¨­å®š
            )
            page = browser.pages[0] if browser.pages else browser.new_page()
            
            # ä¸‹æ›¸ããƒšãƒ¼ã‚¸ã«ç›´æ¥ç§»å‹•ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
            max_attempts = 3
            page_loaded = False
            for attempt in range(max_attempts):
                try:
                    page.goto("https://fril.jp/draft", timeout=30000, wait_until='load')
                    page.wait_for_load_state('networkidle', timeout=10000)
                    page_loaded = True
                    logger.info("âœ… ãƒ©ã‚¯ãƒãƒ‰ãƒ©ãƒ•ãƒˆãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†")
                    break
                except Exception as e:
                    logger.warning(f"  ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿è©¦è¡Œ {attempt+1}/{max_attempts} å¤±æ•—: {e}")
                    if attempt < max_attempts - 1:
                        time.sleep(2)
            
            if not page_loaded:
                logger.warning("  ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™")
            
            time.sleep(1)
            
            # æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ç”¨ã®å¾…æ©Ÿï¼ˆãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾Œã€60ç§’ï¼‰
            wait_for_manual_login(page, "ãƒ©ã‚¯ãƒï¼ˆä¸‹æ›¸ãã‚¿ãƒ–ï¼‰", timeout_seconds=60, is_logged_in_func=is_logged_in_rakuma)
            
            # ã€Œå‡ºå“ã—ã¦ã„ãŸã€ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
            try:
                after_selling_tab = page.locator('a[href="#after-selling-tab"]')
                if after_selling_tab.count() > 0:
                    after_selling_tab.click()
                    time.sleep(1)  # 2ç§’â†’1ç§’ã«çŸ­ç¸®
                    
                    # ã€Œç¶šãã‚’è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æœ€å¤§10å›ã¾ã§ã‚¯ãƒªãƒƒã‚¯ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
                    for _ in range(10):
                        try:
                            more_button = page.locator('#after-selling-container_button a')
                            if more_button.is_visible(timeout=1000):
                                more_button.click()
                                time.sleep(0.5)  # 1ç§’â†’0.5ç§’ã«çŸ­ç¸®
                            else:
                                break
                        except:
                            break
                    
                    # å•†å“ã®ç·¨é›†ãƒªãƒ³ã‚¯ã‚’ä¸€æ‹¬å–å¾—
                    edit_links = page.locator('a[href*="/drafts/"][href*="/edit"]')
                    count = edit_links.count()
                    logger.info(f"  ä¸‹æ›¸ãå•†å“æ•°ï¼ˆå‡ºå“ã—ã¦ã„ãŸï¼‰: {count}")
                    
                    # ä¸€æ‹¬ã§ href ã‚’å–å¾—ã—ã¦å‡¦ç†ï¼ˆå€‹åˆ¥ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ï¼‰
                    hrefs = edit_links.evaluate_all('links => links.map(l => l.href)')
                    for href in hrefs:
                        match = re.search(r'/drafts/([a-f0-9]+)/edit', href)
                        if match:
                            item_id = match.group(1)
                            item_url = f"https://item.fril.jp/{item_id}"
                            draft_urls.append(item_url)
                    
                    logger.info(f"âœ… ä¸‹æ›¸ãã‹ã‚‰ {len(draft_urls)} ä»¶ã®URLã‚’å–å¾—ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"ä¸‹æ›¸ãã‚¿ãƒ–ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—: {e}")
            
            browser.close()
    except Exception as e:
        logger.error(f"ä¸‹æ›¸ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        # ãƒ–ãƒ©ã‚¦ã‚¶ãŒç¢ºå®Ÿã«é–‰ã˜ã‚‹ã‚ˆã†ã«å¾…æ©Ÿ
        time.sleep(2)
    
    return set(draft_urls)

def scrape_mercari_shops_stats():
    """ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ã®å…¬é–‹ã‚¿ãƒ–ã‹ã‚‰watchï¼ˆã„ã„ã­ï¼‰ã¨accessï¼ˆé–²è¦§æ•°ï¼‰ã‚’å–å¾—ã™ã‚‹"""
    from bs4 import BeautifulSoup
    stats_dict = {}  # {å•†å“ID: {'watch': 0, 'access': 0, 'name': å•†å“å, 'price': è²©å£²ä¾¡æ ¼}}
    
    try:
        # user_data_dirã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’ä¿å­˜
        script_dir = os.path.dirname(os.path.abspath(__file__))
        user_data_dir = os.path.join(script_dir, 'mercari_shops_user_data')

        with sync_playwright() as p:
            logger.info("ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ã®å…¬é–‹å•†å“æƒ…å ±ã‚’å–å¾—ä¸­... (Firefox)")
            browser = p.firefox.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,
                timeout=60000,
                accept_downloads=True
            )

            def handle_popup(popup):
                logger.info(f"  ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãŒé–‹ã‹ã‚Œã¾ã—ãŸ: {popup.url}")
            browser.on("page", handle_popup)
            page = browser.pages[0] if browser.pages else browser.new_page()
            
            shop_url = "https://mercari-shops.com/seller/shops/qWxSdPm7yRZ56vy6jEx9mK/products"

            # æœ€åˆã«æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿã‚’å®Ÿè¡Œï¼ˆãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾Œï¼‰
            page.goto(shop_url, timeout=60000, wait_until='load')
            page.wait_for_load_state('networkidle', timeout=10000)
            time.sleep(1)
            wait_for_manual_login(page, "ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹", timeout_seconds=60, is_logged_in_func=is_logged_in_mercari_shops)

            max_retries = 3
            for attempt in range(max_retries):
                page.goto(shop_url, timeout=60000, wait_until='load')
                page.wait_for_load_state('networkidle', timeout=10000)
                time.sleep(2)
                current_url = page.url
                html_content = page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                test_items = soup.find_all('li', {'data-testid': 'product'})
                login_btn = soup.find('button', {'data-testid': 'login-with-mercari-account'})
                # å•†å“è¦ç´ ãŒå­˜åœ¨ã—ã€URLã‚‚æ­£ã—ã„å ´åˆã¯OK
                if (
                    'qWxSdPm7yRZ56vy6jEx9mK' in current_url and
                    'login' not in current_url.lower() and
                    len(test_items) > 0 and not login_btn
                ):
                    logger.info("âœ… å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã«é·ç§»æˆåŠŸã€‚å•†å“æƒ…å ±ã‚’å…¨ä»¶å–å¾—ã—ã¾ã™...")
                    break
                else:
                    logger.warning(f"  [ãƒªãƒˆãƒ©ã‚¤{attempt+1}/{max_retries}] å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸é·ç§»å¤±æ•—ã¾ãŸã¯å•†å“è¦ç´ æœªæ¤œå‡ºã€‚å†é·ç§»ã—ã¾ã™...")
                    time.sleep(3)
            else:
                logger.error(f"âŒ å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã«é·ç§»ã§ãã¦ã„ã¾ã›ã‚“ã€‚URL: {current_url}")
                logger.error("   ãƒšãƒ¼ã‚¸ã«ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒå­˜åœ¨ã—ã¾ã™ã€‚ãƒ­ã‚°ã‚¤ãƒ³å¾Œã«å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                logger.error("   ãƒ–ãƒ©ã‚¦ã‚¶ã¯è‡ªå‹•ã§é–‰ã˜ã¾ã›ã‚“ã€‚æ‰‹å‹•ã§ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã¦ãã ã•ã„ã€‚")
                input("[ENTER]ã§ç¶šè¡Œï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã‚’æ‰‹å‹•ã§é–‰ã˜ãŸå¾Œï¼‰: ")
                return stats_dict

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã™ã¹ã¦ã®å•†å“ã‚’èª­ã¿è¾¼ã‚€
            logger.info("  å•†å“ãƒªã‚¹ãƒˆã‚’è‡ªå‹•ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
            prev_count = 0
            same_count = 0
            max_scroll = 100
            for i in range(max_scroll):
                current_url_during_scroll = page.url
                if 'login' in current_url_during_scroll.lower() or 'signin' in current_url_during_scroll.lower():
                    logger.warning("  âš ï¸ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ã«ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚Œã¾ã—ãŸã€‚ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
                    break
                try:
                    last_item = page.locator('li[data-testid="product"]:last-child')
                    if last_item.count() > 0:
                        last_item.scroll_into_view_if_needed()
                    else:
                        page.evaluate("window.scrollBy(0, 500)")
                except Exception as e:
                    logger.warning(f"  ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
                    page.evaluate("window.scrollBy(0, 500)")
                time.sleep(1.5)
                page.wait_for_load_state('networkidle', timeout=5000)
                html_content = page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                items = soup.find_all('li', {'data-testid': 'product'})
                curr_count = len(items)
                logger.info(f"    ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«{i+1}å›ç›®: å•†å“æ•°={curr_count}")
                if curr_count == prev_count:
                    same_count += 1
                else:
                    same_count = 0
                if same_count >= 5:
                    logger.info(f"    å•†å“æ•°ãŒå¢—ãˆãªããªã£ãŸãŸã‚ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«çµ‚äº†ï¼ˆ{curr_count}ä»¶ï¼‰")
                    break
                prev_count = curr_count
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†å¾Œã®HTMLã‚’ä½¿ç”¨
            browser.close()
            # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLã‚’ä¿å­˜
            debug_html_path = "mercari_shops_debug.html"
            with open(debug_html_path, "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°ç”¨HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {debug_html_path}")
            # BeautifulSoupã§è§£æ
            soup = BeautifulSoup(html_content, 'html.parser')
            items = soup.find_all('li', {'data-testid': 'product'})
            logger.info(f"  å…¬é–‹å•†å“æ•°: {len(items)}")
            # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®å•†å“è¦ç´ ã®å†…å®¹ã‚’ç¢ºèª
            if items:
                first_item = items[0]
                logger.info(f"  [ãƒ‡ãƒãƒƒã‚°] æœ€åˆã®å•†å“è¦ç´ ã®HTML: {str(first_item)[:200]}...")
            else:
                logger.warning("  âš ï¸ å•†å“è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                # ä»–ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
                alt_items = soup.find_all('li', class_=re.compile(r'product'))
                logger.info(f"  ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ã§ã®å•†å“æ•°: {len(alt_items)}")
                if alt_items:
                    items = alt_items
                    logger.info("  ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            
            for idx, item in enumerate(items):
                    try:
                        # å•†å“è¦ç´ ã®HTMLã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæœ€åˆã®10ä»¶ï¼‰
                        if idx < 10:
                            logger.info(f"  [ãƒ‡ãƒãƒƒã‚°] å•†å“è¦ç´ HTML({idx}): {str(item)[:200]}...")

                        # å•†å“å
                        name_tag = item.find('p', {'data-testid': 'product-name'})
                        product_name = name_tag.get_text(strip=True) if name_tag else None
                        if not product_name:
                            logger.warning(f"  å•†å“åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆidx={idx}ï¼‰")

                        # å“ç•ªã‚’æŠ½å‡ºï¼ˆå…ˆé ­ã§ãªãã¦ã‚‚3-5æ¡ã®æ•°å­—ãŒã‚ã‚Œã°OKï¼‰
                        hinban_match = re.search(r'(\d{3,5})', product_name) if product_name else None
                        hinban = hinban_match.group(1) if hinban_match else None
                        # å“ç•ªãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                        if hinban and idx < 10:
                            logger.info(f"  [ãƒ‡ãƒãƒƒã‚°] å“ç•ªã‚’æŠ½å‡º: {hinban}ï¼ˆidx={idx}ï¼‰")

                        # å•†å“IDï¼ˆURLã®æœ«å°¾ï¼‰
                        product_id = None
                        item_url = None

                        # ä¾¡æ ¼
                        price_tag = item.find('span', class_='css-1wgdpkr')
                        price_text = price_tag.get_text(strip=True) if price_tag else None
                        price = int(re.sub(r'[^\d]', '', price_text)) if price_text and re.search(r'\d', price_text) else 0

                        # åœ¨åº«æ•°ã®æŠ½å‡ºï¼ˆè²©å£²ä¸­å•†å“ã®ã¿ï¼‰
                        stock = ''
                        # 1. divæ§‹é€ ã‹ã‚‰å–å¾—ï¼ˆcss-k008qsã‚¯ãƒ©ã‚¹ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                        stock_div = item.find('div', class_='css-k008qs')
                        if stock_div:
                            stock_label = stock_div.find('span', string='åœ¨åº«æ•°')
                            if stock_label:
                                stock_value = stock_div.find('p', class_='chakra-text')
                                if stock_value:
                                    stock = stock_value.get_text(strip=True)
                        # â€» å£²å´æ¸ˆã¿å•†å“ã§ã¯åœ¨åº«æ•°divãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ç©ºã®ã¾ã¾ã«ã™ã‚‹

                        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆä»¶æ•°ã®æŠ½å‡º
                        request_count = 0
                        request_tag = item.find('p', class_='chakra-text css-15vrkfh')
                        if request_tag:
                            request_text = request_tag.get_text(strip=True)
                            request_match = re.search(r'(\d+)', request_text)
                            if request_match:
                                request_count = int(request_match.group(1))

                        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã‚’å–å¾—ï¼ˆã„ã„ã­ã€é–²è¦§æ•°ï¼‰
                        p_tags = item.find_all('p', class_='chakra-text')
                        watch = 0
                        access = 0
                        if stock:  # åœ¨åº«æ•°ãŒã‚ã‚‹å ´åˆï¼ˆè²©å£²ä¸­ï¼‰
                            if len(p_tags) >= 4:
                                watch_text = p_tags[2].get_text(strip=True)
                                if watch_text.isdigit():
                                    watch = int(watch_text)
                                access_text = p_tags[3].get_text(strip=True)
                                if access_text.isdigit():
                                    access = int(access_text)
                        else:  # åœ¨åº«æ•°ãŒãªã„å ´åˆï¼ˆå£²å´æ¸ˆã¿ï¼‰
                            if len(p_tags) >= 3:
                                watch_text = p_tags[1].get_text(strip=True)
                                if watch_text.isdigit():
                                    watch = int(watch_text)
                                access_text = p_tags[2].get_text(strip=True)
                                if access_text.isdigit():
                                    access = int(access_text)

                        # å•†å“åã‚’ã‚­ãƒ¼ã«ã—ã¦æ ¼ç´ï¼ˆé‡è¤‡æ™‚ã¯idxä»˜ãï¼‰
                        dict_key = product_name or f'idx_{idx}'
                        if dict_key in stats_dict:
                            dict_key = f'{dict_key}_{idx}'
                        stats_dict[dict_key] = {
                            'watch': watch,
                            'access': access,
                            'name': product_name,
                            'price': price,
                            'hinban': hinban,
                            'item_url': item_url,
                            'åœ¨åº«æ•°': stock,
                            'ãƒªã‚¯ã‚¨ã‚¹ãƒˆ': request_count
                        }

                        if idx < 3:
                            logger.info(f"  [ãƒ‡ãƒãƒƒã‚°] å•†å“å {product_name[:30]}..., price={price}, watch={watch}, access={access}, hinban={hinban}, stock={stock}, request={request_count}")

                    except Exception as e:
                        logger.warning(f"  å•†å“ã®watch/accesså–å¾—ã‚¨ãƒ©ãƒ¼: {e}, idx={idx}")
                        continue
            
            logger.info(f"âœ… {len(stats_dict)} ä»¶ã®ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹å•†å“æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
    
    return stats_dict

def process_rakuma_data(base_url, scrape_all_pages=True, request_timeout=10, page_sleep=0.6, item_sleep=0.4, rakuma_stats=None):
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒ©ã‚¯ãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ã—ã¦æ•´å½¢ã™ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    # --- ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆé€†å¼•ãç”¨ï¼‰ ---
    CONDITION_MAP_INV = {'æ–°å“ã€æœªä½¿ç”¨': '1', 'æœªä½¿ç”¨ã«è¿‘ã„': '2', 'ç›®ç«‹ã£ãŸå‚·ã‚„æ±šã‚Œãªã—': '3', 'ã‚„ã‚„å‚·ã‚„æ±šã‚Œã‚ã‚Š': '4', 'å‚·ã‚„æ±šã‚Œã‚ã‚Š': '5', 'å…¨ä½“çš„ã«çŠ¶æ…‹ãŒæ‚ªã„': '6'}
    SHIPPING_PAYER_MAP_INV = {'é€æ–™è¾¼ã¿(å‡ºå“è€…è² æ‹…)': '1', 'ç€æ‰•ã„(è³¼å…¥è€…è² æ‹…)': '2', 'é€æ–™è¾¼': '1'}
    DAYS_TO_SHIP_MAP_INV = {'1-2æ—¥ã§ç™ºé€': '1', '2-3æ—¥ã§ç™ºé€': '2', '4-7æ—¥ã§ç™ºé€': '3', 'æ”¯æ‰•ã„å¾Œã€1ï½2æ—¥ã§ç™ºé€': '1', 'æ”¯æ‰•ã„å¾Œã€2ï½3æ—¥ã§ç™ºé€': '2', 'æ”¯æ‰•ã„å¾Œã€4ï½7æ—¥ã§ç™ºé€': '3'}

    logger.info("Processing Rakuma data from web (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰: å•†å“ä¸€è¦§ã®ã¿)...")
    page = 1
    all_products = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    session = build_requests_session(headers)

    while True:
        url = f"{base_url}?page={page}"
        logger.info(f"Scraping Rakuma page: {url}")
        try:
            response = session.get(url, timeout=request_timeout)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching Rakuma page: {e}")
            break

        # ãƒ‡ãƒãƒƒã‚°ç”¨: æœ€åˆã®ãƒšãƒ¼ã‚¸ã®HTMLã‚’ä¿å­˜
        if page == 1:
            debug_html_path = "rakuma_debug.html"
            with open(debug_html_path, "w", encoding="utf-8") as f:
                f.write(response.text)
            logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°ç”¨HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {debug_html_path}")

        soup = BeautifulSoup(response.content, 'html.parser')
        items = soup.find_all('div', class_='item', attrs={'data-test': 'item'})

        if not items:
            logger.info(f"No more items found on page {page}. Stopping.")
            break

        # å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ç›´æ¥æƒ…å ±ã‚’æŠ½å‡ºï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ï¼‰
        for item in items:
            try:
                # å•†å“ãƒªãƒ³ã‚¯
                link_tag = item.find('a', class_='link_shop_image')
                item_url = urljoin(base_url, link_tag['href']) if link_tag and 'href' in link_tag.attrs else ''
                
                # å•†å“åï¼ˆspanã‚¿ã‚°ã® data-test="item_name" ã‹ã‚‰å–å¾—ï¼‰
                name_tag = item.find('span', attrs={'data-test': 'item_name'})
                name = name_tag.text.strip() if name_tag else 'N/A'
                
                # ä¾¡æ ¼ï¼ˆspanã‚¿ã‚°ã® data-test="item_price" ã‹ã‚‰å–å¾—ï¼‰
                price_tag = item.find('span', attrs={'data-test': 'item_price'})
                price = 'N/A'
                if price_tag:
                    price_text = price_tag.get_text(strip=True)
                    price_match = re.search(r'[\d,]+', price_text)
                    if price_match:
                        price = price_match.group(0).replace(',', '')
                
                # SOLD OUTåˆ¤å®šï¼ˆä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
                is_sold_out = item.find('div', class_='soldout') is not None or \
                             item.find('span', class_='soldout') is not None
                
                product_data = {
                    'å•†å“å': name,
                    'ä¾¡æ ¼': price,
                    'URL': item_url,
                    'is_sold_out': is_sold_out,
                    'å•†å“èª¬æ˜': '',  # ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ã¯ç„¡ã„ãŸã‚ç©º
                    'ãƒ–ãƒ©ãƒ³ãƒ‰': '',
                    'ã‚«ãƒ†ã‚´ãƒª': '',
                    'ã‚µã‚¤ã‚º': '',
                    'å•†å“ã®çŠ¶æ…‹': '',
                    'é…é€æ–™ã®è² æ‹…': '',
                    'é…é€æ–¹æ³•': '',
                    'ç™ºé€æ—¥ã®ç›®å®‰': '',
                    'ç™ºé€å…ƒã®åœ°åŸŸ': '',
                }
                all_products.append(product_data)
            except Exception as e:
                logger.warning(f"  Error parsing item: {e}")
                continue
        
        # 1ãƒšãƒ¼ã‚¸ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆã€ã“ã“ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        if not scrape_all_pages:
            logger.info("1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
            break

        page += 1
        time.sleep(page_sleep)  # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã®å¾…æ©Ÿ

    logger.info(f"Found {len(all_products)} products in Rakuma data (ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰).")

    if not all_products:
        logger.warning("No products found in Rakuma data.")
        return pd.DataFrame()

    df = pd.DataFrame(all_products)
    df['å“ç•ª'] = df['å•†å“å'].apply(extract_product_number)
    
    # watch/accessåˆ—ã‚’è¿½åŠ 
    if rakuma_stats:
        logger.info(f"ãƒ©ã‚¯ãƒstatsè¾æ›¸ã« {len(rakuma_stats)} ä»¶ã®æƒ…å ±ãŒã‚ã‚Šã¾ã™")
        df['watch'] = df['URL'].apply(lambda url: rakuma_stats.get(url, {}).get('watch', 0))
        df['access'] = df['URL'].apply(lambda url: rakuma_stats.get(url, {}).get('access', 0))
        
        # ãƒ‡ãƒãƒƒã‚°: ç´ä»˜ã‘æˆåŠŸæ•°ã‚’ç¢ºèª
        matched_count = (df['watch'] > 0).sum() + (df['access'] > 0).sum()
        logger.info(f"ãƒ©ã‚¯ãƒ: watch/accessãŒè¨­å®šã•ã‚ŒãŸè¡Œæ•° = {matched_count}")
        if matched_count == 0 and len(rakuma_stats) > 0:
            logger.warning("âš ï¸ ãƒ©ã‚¯ãƒã®URLç´ä»˜ã‘ã«å¤±æ•—ã—ã¦ã„ã¾ã™")
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            if len(df) > 0:
                logger.info(f"  CSV URLä¾‹: {df['URL'].iloc[0]}")
            if len(rakuma_stats) > 0:
                sample_key = list(rakuma_stats.keys())[0]
                logger.info(f"  stats URLä¾‹: {sample_key}")
    else:
        df['watch'] = 0
        df['access'] = 0
        logger.warning("rakuma_statsãŒç©ºã§ã™")

    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ï¼ˆç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ã§ã¯ç©ºã®ã¾ã¾ã‚‚å¤šã„ï¼‰
    df['å•†å“ã®çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰'] = df['å•†å“ã®çŠ¶æ…‹'].map(CONDITION_MAP_INV)
    df['é…é€æ–™è² æ‹…ã‚³ãƒ¼ãƒ‰'] = df['é…é€æ–™ã®è² æ‹…'].map(SHIPPING_PAYER_MAP_INV)
    df['ç™ºé€æ—¥ã®ç›®å®‰ã‚³ãƒ¼ãƒ‰'] = df['ç™ºé€æ—¥ã®ç›®å®‰'].map(DAYS_TO_SHIP_MAP_INV)

    df = add_duplicate_column(df)
    
    # å‡ºåŠ›ã™ã‚‹åˆ—ã‚’å®šç¾©
    output_cols = [
        'å“ç•ª', 'é‡è¤‡', 'å•†å“å', 'ä¾¡æ ¼', 'URL', 'watch', 'access', 'å•†å“èª¬æ˜', 'ãƒ–ãƒ©ãƒ³ãƒ‰',
        'ã‚«ãƒ†ã‚´ãƒª', 'ã‚µã‚¤ã‚º', 'å•†å“ã®çŠ¶æ…‹', 'é…é€æ–™ã®è² æ‹…', 'é…é€æ–¹æ³•',
        'ç™ºé€æ—¥ã®ç›®å®‰', 'ç™ºé€å…ƒã®åœ°åŸŸ', 'å•†å“ã®çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰', 'é…é€æ–™è² æ‹…ã‚³ãƒ¼ãƒ‰', 'ç™ºé€æ—¥ã®ç›®å®‰ã‚³ãƒ¼ãƒ‰', 'is_sold_out'
    ]
    # å­˜åœ¨ã—ãªã„åˆ—ãŒã‚ã‚Œã°ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
    for col in output_cols:
        if col not in df.columns:
            df[col] = ''

    df = df[output_cols]
    logger.info(f"Rakuma data processing complete: {len(df)} products.")
    return df

def process_mercari_data(mercari_path=None, mercari_stats=None):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã®æœ€æ–°ã®Mercari CSVã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€æ•´å½¢ã™ã‚‹"""
    logger.info("Processing Mercari data...")
    try:
        if mercari_path is None:
            mercari_path = r'C:\Users\progr\Desktop\Python\mercari_dorekai\downloads'
        search_pattern = os.path.join(mercari_path, 'product_data_*.csv')
        
        files = glob.glob(search_pattern)
        if not files:
            logger.warning(f"No Mercari CSV files found at: {search_pattern}")
            return pd.DataFrame(), {} # [ä¿®æ­£] ç©ºã®è¾æ›¸ã‚‚è¿”ã™
        
        latest_file = max(files, key=os.path.getmtime)
        logger.info(f"Processing latest Mercari file: {latest_file}")
        
        df = pd.read_csv(latest_file, encoding='cp932')
        
        df = df.rename(columns={'è²©å£²ä¾¡æ ¼': 'ä¾¡æ ¼'})
        
        # åˆ—åã®æºã‚Œã«å¯¾å¿œ
        id_candidates = ['å•†å“ID', 'å•†å“ID(å¿…é ˆ)', 'å•†å“id', 'item_id']
        id_col = next((c for c in id_candidates if c in df.columns), None)
        if id_col:
            df.rename(columns={id_col: 'å•†å“ID'}, inplace=True)
        else:
            logger.warning("'å•†å“ID' column not found in Mercari CSV.")

        # å“ç•ªã¯æ•°å­—ã®ã¿æŠ½å‡º
        hinban_series = df['å•†å“å'].apply(extract_product_number)

        # URLåˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        if 'URL' in df.columns:
            url_series = df['URL']
        else:
            url_series = [''] * len(df)

        # watch/accessåˆ—ã‚’è¿½åŠ ï¼ˆå•†å“åå…¨ä½“ã§ãƒãƒƒãƒãƒ³ã‚°ï¼‰
        watch_series = [0] * len(df)
        access_series = [0] * len(df)
        if mercari_stats:
            logger.info(f"ãƒ¡ãƒ«ã‚«ãƒªstatsè¾æ›¸ã« {len(mercari_stats)} ä»¶ã®æƒ…å ±ãŒã‚ã‚Šã¾ã™")
            matched_count = 0
            for idx, product_name in enumerate(df['å•†å“å'].astype(str)):
                if product_name in mercari_stats:
                    watch_series[idx] = mercari_stats[product_name].get('watch', 0)
                    access_series[idx] = mercari_stats[product_name].get('access', 0)
                    matched_count += 1
            logger.info(f"ãƒ¡ãƒ«ã‚«ãƒª: {matched_count} ä»¶ã®å•†å“åãŒstatsã¨ç´ä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ")
            if matched_count == 0 and len(mercari_stats) > 0:
                logger.warning("âš ï¸ ãƒ¡ãƒ«ã‚«ãƒªã®å•†å“åç´ä»˜ã‘ã«å¤±æ•—ã—ã¦ã„ã¾ã™")
                if len(df) > 0:
                    logger.info(f"  CSVå•†å“åä¾‹: {df['å•†å“å'].iloc[0]}")
                if len(mercari_stats) > 0:
                    sample_key = list(mercari_stats.keys())[0]
                    logger.info(f"  statså•†å“åä¾‹: {sample_key}")
        else:
            logger.warning("mercari_statsãŒç©ºã§ã™")
        # è¤‡æ•°åˆ—ã‚’ä¸€åº¦ã«è¿½åŠ ï¼ˆæœ€é©åŒ–ï¼‰
        # æ³¨æ„: å•†å“ç™»éŒ²æ—¥æ™‚ã¨æœ€çµ‚æ›´æ–°æ—¥æ™‚ã¯å…ƒã®CSVã«æ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚è¿½åŠ ã—ãªã„
        new_cols = pd.DataFrame({
            'URL': url_series,
            'å“ç•ª': hinban_series,
            'watch': watch_series,
            'access': access_series
        }, index=df.index)
        df = pd.concat([df, new_cols], axis=1)
        df = add_duplicate_column(df)

        # å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        if 'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹' not in df.columns:
            df['å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'] = '0'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦ '0' (è²©å£²ä¸­) ã‚’è¨­å®š
        
        # [è¿½åŠ ] ãƒ–ãƒ©ãƒ³ãƒ‰IDã¨å“ç•ªã®ãƒãƒƒãƒ—ã‚’ä½œæˆ
        hinban_to_brandid_map = {}
        if 'å“ç•ª' in df.columns and 'ãƒ–ãƒ©ãƒ³ãƒ‰ID' in df.columns:
            df_map = df[['å“ç•ª', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID']].copy()
            df_map.dropna(subset=['å“ç•ª', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID'], inplace=True)
            df_map['å“ç•ª'] = df_map['å“ç•ª'].astype(str)
            hinban_to_brandid_map = pd.Series(df_map['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].values, index=df_map['å“ç•ª']).to_dict()
            print(f"ğŸ“š å“ç•ª->ãƒ–ãƒ©ãƒ³ãƒ‰IDè¾æ›¸ã‚’ {len(hinban_to_brandid_map)} ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

        # åœ¨åº«æ•°ã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ—ã‚’è¿½åŠ ï¼ˆmercari_statsã‹ã‚‰å–å¾—ï¼‰
        stock_series = ['' for _ in range(len(df))]
        request_series = [0 for _ in range(len(df))]
        if mercari_stats:
            for idx, product_name in enumerate(df['å•†å“å'].astype(str)):
                if product_name in mercari_stats:
                    stats_info = mercari_stats[product_name]
                    stock_series[idx] = stats_info.get('åœ¨åº«æ•°', '')
                    request_series[idx] = stats_info.get('ãƒªã‚¯ã‚¨ã‚¹ãƒˆ', 0)
        
        stock_col = pd.DataFrame({'åœ¨åº«æ•°': stock_series}, index=df.index)
        request_col = pd.DataFrame({'ãƒªã‚¯ã‚¨ã‚¹ãƒˆ': request_series}, index=df.index)
        df = pd.concat([df, stock_col, request_col], axis=1)

        final_cols = ['å“ç•ª', 'é‡è¤‡', 'å•†å“å', 'ä¾¡æ ¼', 'åœ¨åº«æ•°', 'watch', 'access', 'ãƒªã‚¯ã‚¨ã‚¹ãƒˆ', 'å•†å“ç™»éŒ²æ—¥æ™‚', 'æœ€çµ‚æ›´æ–°æ—¥æ™‚', 'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'å•†å“ID', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID']
        for col in final_cols:
            if col not in df.columns:
                df[col] = ''
        
        logger.info(f"Found {len(df)} products in Mercari data.")
        return df[final_cols].copy(), hinban_to_brandid_map

    except Exception as e:
        logger.error(f"An error occurred while processing Mercari data: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚æœ€ä½é™ã®å•†å“åãƒ»ä¾¡æ ¼ãƒ»watch/accessåˆ—ã§CSVå‡ºåŠ›
        try:
            if 'å•†å“å' in df.columns:
                minimal_cols = ['å•†å“å', 'ä¾¡æ ¼']
                if 'watch' in df.columns and 'access' in df.columns:
                    minimal_cols += ['watch', 'access']
                minimal_df = df[minimal_cols].copy()
                # é‡è¤‡åˆ—ã‚’è¿½åŠ 
                minimal_df = add_duplicate_column(minimal_df)
                logger.warning("ã‚¨ãƒ©ãƒ¼æ™‚ã®ç°¡æ˜“CSVå‡ºåŠ›ï¼ˆé‡è¤‡åˆ—ä»˜ãï¼‰ã‚’å®Ÿæ–½")
                return minimal_df, {}
        except Exception as e2:
            logger.error(f"ç°¡æ˜“CSVå‡ºåŠ›ã‚‚å¤±æ•—: {e2}")
        return pd.DataFrame(), {}

def convert_to_edit_url(url):
    """URLã‚’ç·¨é›†ãƒšãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›"""
    if '/edit' in url:
        return url
    
    if 'item.fril.jp/' in url:
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        base_url = url.split('?')[0]
        # item.fril.jp/{id} ã‹ã‚‰ {id} ã‚’æŠ½å‡º
        parts = base_url.split('item.fril.jp/')
        if len(parts) > 1:
            item_id = parts[1].rstrip('/')
            # fril.jp/item/{id}/edit å½¢å¼ã«å¤‰æ›
            return f"https://fril.jp/item/{item_id}/edit"
    
    return url

def move_to_draft_auto(draft_urls):
    """ã€è‡ªå‹•åŒ–ç‰ˆã€‘ãƒ©ã‚¯ãƒã®å•†å“ã‚’ä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹"""
    if not draft_urls:
        logger.info("âœ… ä¸‹æ›¸ãç§»åŠ¨å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“")
        return 0
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    user_data_dir = os.path.join(script_dir, 'rakuma_user_data')
    
    # URLã‚’ç·¨é›†ãƒšãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
    edit_urls = [convert_to_edit_url(url) for url in draft_urls]
    
    logger.info(f"\nğŸ“ {len(edit_urls)} ä»¶ã®å•†å“ã‚’ä¸‹æ›¸ãã«ç§»å‹•ã—ã¾ã™")
    
    success_count = 0
    fail_count = 0
    
    try:
        with sync_playwright() as p:
            # Chromiumãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ï¼ˆæ—¢å­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨åŒã˜profileä½¿ç”¨ï¼‰
            logger.info("ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­ï¼ˆä¸‹æ›¸ãç§»åŠ¨ç”¨ï¼‰...")
            browser = p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,
                timeout=60000
            )
            
            page = browser.pages[0] if browser.pages else browser.new_page()
            
            # ãƒã‚¤ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºèª
            logger.info("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
            try:
                page.goto("https://fril.jp/mypage", timeout=30000, wait_until='domcontentloaded')
                time.sleep(2)
            except:
                logger.warning("  ãƒã‚¤ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã€‚ç¶šè¡Œã—ã¾ã™...")
            
            current_url = page.url
            if "login" in current_url.lower():
                logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ‡ã‚Œï¼‰")
                browser.close()
                return 0
            
            logger.info("âœ… ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ç¢ºèªå®Œäº†")
            
            for idx, url in enumerate(edit_urls, 1):
                logger.info(f"[{idx}/{len(edit_urls)}] å‡¦ç†ä¸­: {url}")
                
                try:
                    # å•†å“ç·¨é›†ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                    page.goto(url, timeout=30000, wait_until='domcontentloaded')
                    time.sleep(2)
                    
                    current_url = page.url
                    
                    # 404ãƒã‚§ãƒƒã‚¯
                    if page.locator('h1:has-text("ãŠæ¢ã—ã®ãƒšãƒ¼ã‚¸")').count() > 0:
                        logger.info(f"  âš ï¸ ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰")
                        success_count += 1
                        continue
                    
                    # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                    if "login" in current_url.lower():
                        logger.warning(f"  âŒ ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ")
                        fail_count += 1
                        continue
                    
                    # ã€Œä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒƒã‚¯
                    draft_button = None
                    try:
                        draft_button = page.locator('button:has-text("ä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹")').first
                        if draft_button.count() > 0:
                            draft_button.click(timeout=5000)
                            logger.info(f"  ğŸ“ ã€Œä¸‹æ›¸ãã«ä¿å­˜ã™ã‚‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯")
                    except:
                        pass
                    
                    if not draft_button:
                        logger.warning(f"  âš ï¸ ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆæ—¢ã«ä¸‹æ›¸ã or å£²å´æ¸ˆã¿ï¼‰")
                        success_count += 1
                        continue
                    
                    # ç¢ºèªãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
                    time.sleep(1)
                    try:
                        confirm_button = page.locator('button:has-text("ä¸‹æ›¸ãã«æˆ»ã™")').first
                        if confirm_button.count() > 0:
                            confirm_button.click(timeout=5000)
                            logger.info(f"  âœ… ä¸‹æ›¸ãã«ç§»å‹•ã—ã¾ã—ãŸ")
                            success_count += 1
                            time.sleep(2)
                        else:
                            logger.warning(f"  âš ï¸ ç¢ºèªãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            fail_count += 1
                    except Exception as e:
                        logger.warning(f"  âš ï¸ ç¢ºèªãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
                        fail_count += 1
                    
                except Exception as e:
                    logger.warning(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    fail_count += 1
            
            browser.close()
    
    except Exception as e:
        logger.error(f"âŒ ãƒ‰ãƒ©ãƒ•ãƒˆç§»åŠ¨å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return 0
    
    logger.info(f"\nğŸ“Š ãƒ‰ãƒ©ãƒ•ãƒˆç§»åŠ¨å®Œäº†: æˆåŠŸ {success_count} ä»¶ / å¤±æ•— {fail_count} ä»¶")
    return success_count

def parse_args():
    parser = argparse.ArgumentParser(description="Rakuma/Mercari data scraper")
    parser.add_argument("--base-url", default="https://fril.jp/shop/3c65d78bc0e1eadbe2a3528b344d8311")
    parser.add_argument("--scrape-all-pages", action="store_true", default=True)
    parser.add_argument("--mercari-path", default=None)
    parser.add_argument("--page-sleep", type=float, default=0.6)
    parser.add_argument("--item-sleep", type=float, default=0.4)
    parser.add_argument("--request-timeout", type=int, default=10)
    parser.add_argument("--verbose", action="store_true")
    return parser.parse_args()

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    args = parse_args()
    configure_logging(args.verbose)

    # å…ˆã«ãƒ©ã‚¯ãƒã®å‡ºå“ä¸­å•†å“ã®watch/accessã‚’å–å¾—
    logger.info("\n=== ãƒ©ã‚¯ãƒå‡ºå“ä¸­å•†å“ã®watch/accesså–å¾— ===")
    rakuma_stats = scrape_rakuma_selling_stats()
    logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹ã®å®Œå…¨çµ‚äº†ã‚’å¾…æ©Ÿä¸­...")
    time.sleep(5)  # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œå…¨ã«çµ‚äº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
    
    # ä¸‹æ›¸ãã‚¿ãƒ–ã®å•†å“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ©ã‚¯ãƒãªã®ã§åŒã˜ãƒ–ãƒ©ã‚¦ã‚¶ä½¿ç”¨å¯èƒ½ï¼‰
    logger.info("\n=== ãƒ©ã‚¯ãƒä¸‹æ›¸ãã‚¿ãƒ–ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ===")
    draft_urls = scrape_rakuma_draft_items()
    logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹ã®å®Œå…¨çµ‚äº†ã‚’å¾…æ©Ÿä¸­...")
    time.sleep(5)  # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œå…¨ã«çµ‚äº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
    
    # ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ã®å…¬é–‹å•†å“ã®watch/accessã‚’å–å¾—ï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
    logger.info("\n=== ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹å…¬é–‹å•†å“ã®watch/accesså–å¾— ===")
    mercari_stats = scrape_mercari_shops_stats()
    logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹ã®å®Œå…¨çµ‚äº†ã‚’å¾…æ©Ÿä¸­...")
    time.sleep(3)  # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œå…¨ã«çµ‚äº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ

    logger.info("\n=== ãƒ©ã‚¯ãƒå•†å“ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç† ===")
    # æ—¢ã«å–å¾—ã—ãŸãƒ©ã‚¯ãƒã®watch/accessãƒ‡ãƒ¼ã‚¿ã¨draft URLsã‚’ä½¿ç”¨ã—ã¦DataFrameã‚’æ§‹ç¯‰
    logger.info(f"ãƒ©ã‚¯ãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå–å¾—æ¸ˆã¿stats: {len(rakuma_stats)}ä»¶, draft URLs: {len(draft_urls)}ä»¶ï¼‰")
    
    if rakuma_stats:
        # rakuma_stats ã‹ã‚‰ DataFrame ã‚’ç›´æ¥æ§‹ç¯‰
        # ãŸã ã— stats ã«ã¯å•†å“åãŒå«ã¾ã‚Œãªã„ãŸã‚ã€å„URLã‹ã‚‰å•†å“åã‚’è»½é‡ã«å–å¾—ã—ã¦å“ç•ªã‚’æŠ½å‡º
        logger.info("rakuma_stats ãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€URLä¸€è¦§ã‹ã‚‰å•†å“åã‚’å–å¾—ã—ã¦ DataFrame ã‚’æ§‹ç¯‰ã—ã¾ã™")
        rakuma_data_list = []

        # è»½é‡ãª requests ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        session = build_requests_session(headers)

        for url, stats_info in rakuma_stats.items():
            name = ''
            try:
                # è»½ã„ GET ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ã‚ï¼‰
                resp = session.get(url, timeout=5)
                if resp.status_code == 200:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    # å„ªå…ˆ: og:title -> data-test item_name -> title
                    og = soup.find('meta', property='og:title')
                    if og and og.get('content'):
                        name = og.get('content')
                    else:
                        name_tag = soup.find(attrs={'data-test': 'item_name'})
                        if name_tag:
                            name = name_tag.get_text(strip=True)
                        else:
                            if soup.title and soup.title.string:
                                name = soup.title.string.strip()
            except Exception:
                # å–å¾—å¤±æ•—ã§ã‚‚ç¶šè¡Œï¼ˆç©ºã® name ã§ã‚‚å•é¡Œãªã„ï¼‰
                name = ''

            name = clean_rakuma_title(name)

            rakuma_data_list.append({
                'å•†å“å': name,
                'URL': url,
                'watch': stats_info.get('watch', 0),
                'access': stats_info.get('access', 0)
            })

        rakuma_df = pd.DataFrame(rakuma_data_list)
        # å“ç•ªã‚’æŠ½å‡º
        if 'å•†å“å' in rakuma_df.columns:
            rakuma_df['å“ç•ª'] = rakuma_df['å•†å“å'].apply(extract_product_number)
        else:
            rakuma_df['å“ç•ª'] = ''

        # é‡è¤‡åˆ—ã‚’è¿½åŠ 
        rakuma_df = add_duplicate_column(rakuma_df)

        logger.info(f"âœ… ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸï¼ˆ{len(rakuma_df)}ä»¶ï¼‰")
    else:
        logger.warning("ãƒ©ã‚¯ãƒã®å–å¾—ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        rakuma_df = pd.DataFrame()
    
    logger.info("\n=== ãƒ¡ãƒ«ã‚«ãƒªå•†å“ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç† ===")
    mercari_df, hinban_to_brandid_map = process_mercari_data(
        mercari_path=args.mercari_path,
        mercari_stats=mercari_stats
    )
    
    # ãƒ¡ãƒ«ã‚«ãƒªã‚·ãƒ§ãƒƒãƒ—ã‚¹ã®statsã¯products_mercari.csvã«çµ±åˆæ¸ˆã¿
    
    brand_master_map = load_brand_master_map()

    # --- [æ–°è¦] ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰å¼•ã„ãŸãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¨­å®š ---
    if not rakuma_df.empty and hinban_to_brandid_map and brand_master_map:
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'] = rakuma_df['å“ç•ª'].astype(str).map(hinban_to_brandid_map)

        # ãƒ–ãƒ©ãƒ³ãƒ‰IDã«åŸºã¥ã„ã¦å„ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¨­å®šã™ã‚‹é–¢æ•°
        def get_brand_details(brand_id, column_name):
            if pd.isna(brand_id):
                return None
            return brand_master_map.get(str(brand_id), {}).get(column_name, None)

        # æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰å'] = rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].apply(lambda x: get_brand_details(x, 'ãƒ–ãƒ©ãƒ³ãƒ‰å'))
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰'] = rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].apply(lambda x: get_brand_details(x, 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰'))
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰'] = rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].apply(lambda x: get_brand_details(x, 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰'))
        # [å‰Šé™¤] æ—¢å­˜ã®ãƒ–ãƒ©ãƒ³ãƒ‰åˆ—ã®æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤

    # --- ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«ãƒ¡ãƒ«ã‚«ãƒªã®å•†å“IDã‚’ç´ä»˜ã‘ã‚‹å‡¦ç†ã‚’è¿½åŠ  ---
    logger.info("ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«ãƒ¡ãƒ«ã‚«ãƒªã®å•†å“IDã‚’ç´ä»˜ã‘ã¾ã™...")
    if not rakuma_df.empty and not mercari_df.empty and 'å“ç•ª' in mercari_df.columns and 'å•†å“ID' in mercari_df.columns:
        # ãƒ¡ãƒ«ã‚«ãƒªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å“ç•ªã¨å•†å“IDã®ã¿ã‚’æŠ½å‡ºï¼ˆé‡è¤‡ã¯æœ€åˆã®ã‚‚ã®ã‚’æ¡ç”¨ï¼‰
        mercari_id_map = mercari_df.drop_duplicates(subset=['å“ç•ª'])[['å“ç•ª', 'å•†å“ID']].copy()
        # å“ç•ªã‚’æ–‡å­—åˆ—ã«çµ±ä¸€ã—ã¦ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
        mercari_id_map['å“ç•ª'] = mercari_id_map['å“ç•ª'].astype(str)
        if 'å“ç•ª' in rakuma_df.columns:
            rakuma_df['å“ç•ª'] = rakuma_df['å“ç•ª'].astype(str)
            # ãƒ©ã‚¯ãƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ¡ãƒ«ã‚«ãƒªã®æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
            rakuma_df = pd.merge(rakuma_df, mercari_id_map, on='å“ç•ª', how='left')
            # ãƒãƒ¼ã‚¸ã«ã‚ˆã£ã¦ 'å•†å“ID_x', 'å•†å“ID_y' ãŒã§ãã‚‹ã®ã‚’é˜²ããŸã‚ã€å…ƒã® 'å•†å“ID' ã‚’å„ªå…ˆ
            rakuma_df.rename(columns={'å•†å“ID_x': 'å•†å“ID'}, inplace=True)
            logger.info("ãƒ¡ãƒ«ã‚«ãƒªå•†å“IDã®ç´ä»˜ã‘ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        else:
            logger.warning("ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã« 'å“ç•ª' åˆ—ãŒãªã„ãŸã‚ã€ãƒãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    elif rakuma_df.empty:
        logger.warning("ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ãƒ¡ãƒ«ã‚«ãƒªã¨ã®ãƒãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    # ---------------------------------------------------------
    
    logger.info("ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«å‰Šé™¤åˆ—ã‚’è¿½åŠ ã™ã‚‹å‡¦ç†ã‚’é–‹å§‹...")
    
    # ä¸‹æ›¸ãã«æ—¢ã«ã‚ã‚‹å•†å“ã¯å‡¦ç†å¯¾è±¡ã‹ã‚‰é™¤å¤–
    if draft_urls and not rakuma_df.empty:
        original_count = len(rakuma_df)
        rakuma_df = rakuma_df[~rakuma_df['URL'].isin(draft_urls)]
        excluded_count = original_count - len(rakuma_df)
        if excluded_count > 0:
            logger.info(f"âœ… ä¸‹æ›¸ãã«æ—¢ã«ã‚ã‚‹ {excluded_count} ä»¶ã®å•†å“ã‚’å‡¦ç†å¯¾è±¡ã‹ã‚‰é™¤å¤–ã—ã¾ã—ãŸ")
    
    # Mercariã®å“ç•ªã‚’ã‚­ãƒ¼ã€å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
    if not rakuma_df.empty and 'å“ç•ª' in mercari_df.columns and 'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹' in mercari_df.columns:
        # NaNã‚’è€ƒæ…®ã—ã€dropna()ã‚’è¿½åŠ ã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¯æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
        mercari_status_map = pd.Series(mercari_df['å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'].astype(str).values, index=mercari_df['å“ç•ª']).dropna().to_dict()
    else:
        logger.warning("Mercariãƒ‡ãƒ¼ã‚¿ã«'å“ç•ª'ã¾ãŸã¯'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'åˆ—ãŒãªã„ãŸã‚ã€å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        mercari_status_map = {}

    def get_delete_status(row):
        # ãƒ©ã‚¯ãƒã§SOLD OUTã®å ´åˆã¯å‰Šé™¤ã—ãªã„
        if row.get('is_sold_out', False):
            return ''

        hinban = row['å“ç•ª']
        # å“ç•ªãŒNaNã‚„Noneã®å ´åˆã¯ãƒã‚§ãƒƒã‚¯ã—ãªã„
        if pd.isna(hinban):
            return ''
        
        mercari_status = mercari_status_map.get(str(hinban))
        
        if mercari_status is None: # æ¡ä»¶1: Mercariã«å“ç•ªãŒå­˜åœ¨ã—ãªã„
            return 'å‰Šé™¤'

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ '1' (å£²åˆ‡ã‚Œ) ã®å ´åˆ
        if str(mercari_status) == '1': # æ¡ä»¶2: Mercariã§ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ'1'
            return 'å‰Šé™¤'
            
        return ''

    # 'å“ç•ª'åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‰Šé™¤åˆ—ã‚’è¿½åŠ 
    if 'å“ç•ª' in rakuma_df.columns:
        # is_sold_outåˆ—ã‚’å…ˆã«å‡¦ç†
        if 'is_sold_out' not in rakuma_df.columns:
            rakuma_df['is_sold_out'] = False
        else:
            rakuma_df['is_sold_out'] = rakuma_df['is_sold_out'].fillna(False)

        rakuma_df['å‰Šé™¤'] = rakuma_df.apply(get_delete_status, axis=1)
        # is_sold_outåˆ—ã‚’å‰Šé™¤
        if 'is_sold_out' in rakuma_df.columns:
            rakuma_df = rakuma_df.drop(columns=['is_sold_out'])

        # --- åˆ—ã®é †åºã‚’æœ€çµ‚èª¿æ•´ ---
        # åŸºæœ¬ã¨ãªã‚‹åˆ—ã®é †åºã‚’å®šç¾©
        final_cols_order = [
            'å“ç•ª', 'é‡è¤‡', 'å•†å“ID', 'å‰Šé™¤', 'å•†å“å', 'ä¾¡æ ¼', 'URL', 'watch', 'access', 'å•†å“èª¬æ˜', 'ãƒ–ãƒ©ãƒ³ãƒ‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰',
            'ã‚«ãƒ†ã‚´ãƒª', 'ã‚µã‚¤ã‚º', 'å•†å“ã®çŠ¶æ…‹', 'é…é€æ–™ã®è² æ‹…', 'é…é€æ–¹æ³•', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID',
            'ç™ºé€æ—¥ã®ç›®å®‰', 'ç™ºé€å…ƒã®åœ°åŸŸ', 'å•†å“ã®çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰', 'é…é€æ–™è² æ‹…ã‚³ãƒ¼ãƒ‰', 'ç™ºé€æ—¥ã®ç›®å®‰ã‚³ãƒ¼ãƒ‰'
        ]
        
        # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã§é †åºã‚’å†æ§‹ç¯‰
        current_cols = rakuma_df.columns.tolist()
        ordered_cols = [col for col in final_cols_order if col in current_cols]
        
        # é †åºå®šç¾©ã«å«ã¾ã‚Œãªã„ãŒã€ä¸‡ãŒä¸€å­˜åœ¨ã™ã‚‹åˆ—ãŒã‚ã‚Œã°æœ«å°¾ã«è¿½åŠ 
        ordered_cols.extend([col for col in current_cols if col not in ordered_cols])
        
        rakuma_df = rakuma_df[ordered_cols]
    else:
        rakuma_df['å‰Šé™¤'] = ''
        logger.warning("Rakumaãƒ‡ãƒ¼ã‚¿ã«'å“ç•ª'åˆ—ãŒãªã„ãŸã‚ã€å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    logger.info("å‰Šé™¤åˆ—ã®å‡¦ç†å®Œäº†ã€‚")

    # --- æ³¨æ„: ä¸‹æ›¸ãå‡¦ç†ã¯ rakuma_draft_mover.py ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ ---
    delete_count = len(rakuma_df[rakuma_df['å‰Šé™¤'] == 'å‰Šé™¤']) if 'å‰Šé™¤' in rakuma_df.columns else 0
    duplicate_count = 0
    if 'é‡è¤‡' in rakuma_df.columns and 'å“ç•ª' in rakuma_df.columns:
        dup_df = rakuma_df[rakuma_df['é‡è¤‡'] == 'é‡è¤‡']
        if not dup_df.empty:
            duplicate_count = dup_df['å“ç•ª'].nunique()
    
    if delete_count > 0 or duplicate_count > 0:
        print(f"\nğŸ“‹ å‰Šé™¤å¯¾è±¡: {delete_count} ä»¶")
        print(f"ğŸ“‹ é‡è¤‡å¯¾è±¡: {duplicate_count} å“ç•ª")
        print("ğŸ’¡ ä¸‹æ›¸ãã«ç§»å‹•ã™ã‚‹ã«ã¯: python rakuma_draft_mover.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n")

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç½®ã‹ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
    script_dir = os.path.dirname(os.path.abspath(__file__))

    output_rakuma_file = os.path.join(script_dir, 'products_rakuma.csv')
    output_mercari_file = os.path.join(script_dir, 'products_mercari.csv')
    
    logger.info(f"Writing Rakuma data to '{output_rakuma_file}'...")
    rakuma_df.to_csv(output_rakuma_file, index=False, encoding='utf-8-sig')
    
    logger.info(f"Writing Mercari data to '{output_mercari_file}'...")
    mercari_df.to_csv(output_mercari_file, index=False, encoding='utf-8-sig')
    
    # --- ãƒ¤ãƒ•ã‚ªã‚¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ---
    if YAHOOKU_AVAILABLE:
        logger.info("\n=== ãƒ¤ãƒ•ã‚ªã‚¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ ===")
        try:
            driver = setup_driver()
            all_yahooku_items = []
            
            # å‡ºå“ä¸­ã®å•†å“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            logger.info("[1/2] å‡ºå“ä¸­ã®å•†å“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°...")
            items_selling = scrape_url(driver, SELLING_URL, "å‡ºå“ä¸­")
            all_yahooku_items.extend(items_selling)
            
            # çµ‚äº†å•†å“ï¼ˆè½æœ­è€…ãªã—ï¼‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            logger.info("[2/2] çµ‚äº†å•†å“ï¼ˆè½æœ­è€…ãªã—ï¼‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°...")
            items_closed = scrape_url(driver, CLOSED_URL, "çµ‚äº†ï¼ˆè½æœ­è€…ãªã—ï¼‰")
            all_yahooku_items.extend(items_closed)
            
            # CSVã«ä¿å­˜
            save_to_csv(all_yahooku_items)
            logger.info(f"âœ… ãƒ¤ãƒ•ã‚ªã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ products_yahooku.csv ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆ{len(all_yahooku_items)}ä»¶ï¼‰")
            
            # ãƒ‰ãƒ©ã‚¤ãƒã‚’é–‰ã˜ã‚‹
            driver.quit()
            logger.info("ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
            
        except Exception as e:
            logger.error(f"ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        logger.warning("ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    
    # === ã€è‡ªå‹•åŒ–ã€‘ãƒ‰ãƒ©ãƒ•ãƒˆç§»åŠ¨å‡¦ç† ===
    logger.info("\n=== ã€è‡ªå‹•åŒ–ã€‘ãƒ©ã‚¯ãƒå•†å“ã®ä¸‹æ›¸ãç§»åŠ¨ã‚’é–‹å§‹ ===")
    
    if not rakuma_df.empty and 'å‰Šé™¤' in rakuma_df.columns and 'URL' in rakuma_df.columns and 'é‡è¤‡' in rakuma_df.columns:
        # å‰Šé™¤å¯¾è±¡ã¨é‡è¤‡å¯¾è±¡ã‚’åˆã‚ã›ã¦URLæŠ½å‡º
        draft_target_urls = []
        
        # å‰Šé™¤å¯¾è±¡ï¼ˆå‰Šé™¤ == 'å‰Šé™¤'ï¼‰
        delete_targets = rakuma_df[rakuma_df['å‰Šé™¤'] == 'å‰Šé™¤']['URL'].dropna().tolist()
        draft_target_urls.extend(delete_targets)
        
        # é‡è¤‡å¯¾è±¡ï¼ˆé‡è¤‡ == 'é‡è¤‡' ã§ã€å“ç•ªã”ã¨ã«è¤‡æ•°ä»¶ã‚ã‚‹å ´åˆã¯æœ€å¾Œã®ã‚‚ã®ã ã‘æ®‹ã™ï¼‰
        if 'å“ç•ª' in rakuma_df.columns:
            dup_df = rakuma_df[rakuma_df['é‡è¤‡'] == 'é‡è¤‡'].copy()
            if not dup_df.empty:
                # å“ç•ªã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã€æœ€å¾Œã®1ä»¶ã ã‘ã‚’å¯¾è±¡ã¨ã™ã‚‹
                dup_targets = (
                    dup_df.dropna(subset=['URL'])
                          .groupby('å“ç•ª', as_index=False)
                          .tail(1)
                          ['URL']
                          .tolist()
                )
                draft_target_urls.extend(dup_targets)
        
        # é‡è¤‡å‰Šé™¤ï¼ˆåŒã˜URLãŒå‰Šé™¤å¯¾è±¡ãƒ»é‡è¤‡å¯¾è±¡ä¸¡æ–¹ã«å«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ï¼‰
        draft_target_urls = list(set(draft_target_urls))
        
        if draft_target_urls:
            logger.info(f"ğŸ“‹ ä¸‹æ›¸ãç§»åŠ¨å¯¾è±¡: {len(draft_target_urls)} ä»¶")
            
            # æ‰‹å‹•ç¢ºèªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            logger.info("æº–å‚™å®Œäº†ã€‚ä¸‹æ›¸ãç§»åŠ¨ã‚’é–‹å§‹ã—ã¾ã™...")
            time.sleep(2)
            
            # ãƒ‰ãƒ©ãƒ•ãƒˆç§»åŠ¨å®Ÿè¡Œ
            try:
                move_to_draft_auto(draft_target_urls)
            except Exception as e:
                logger.error(f"âŒ ãƒ‰ãƒ©ãƒ•ãƒˆç§»åŠ¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒë°œìƒã—ã¾ã—ãŸ: {e}")
                logger.warning("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯å®Œäº†ã—ã¾ã—ãŸã€‚ä¸‹æ›¸ãç§»åŠ¨ã¯å¤±æ•—ã—ã¾ã—ãŸãŒã€products_rakuma.csv ã¯å‡ºåŠ›æ¸ˆã¿ã§ã™ã€‚")
        else:
            logger.info("âœ… ä¸‹æ›¸ãç§»åŠ¨å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“")
    else:
        logger.warning("âš ï¸ ãƒ©ã‚¯ãƒå•†å“ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ãªãŸã‚ã€ä¸‹æ›¸ãç§»åŠ¨ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
    logger.info("Script finished successfully.")

if __name__ == '__main__':
    main()
