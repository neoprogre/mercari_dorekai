from playwright.sync_api import sync_playwright
from urllib.parse import urlparse
import os, time, requests, json, glob, csv, re, shutil

# === è¨­å®š =========================
# SHOP_URL ã¯ã‚‚ã†ä½¿ã„ã¾ã›ã‚“ï¼ˆå€‹åˆ¥å•†å“ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚ï¼‰
IMAGE_DIR = r"\\LS210DNBD82\share\å¹³è‰¯\Python\mercari_dorekai\mercari_images"
BROKEN_IMAGE_DIR = r"\\LS210DNBD82\share\å¹³è‰¯\Python\mercari_dorekai\mercari_images\ç ´æ"
# product_data_*.csv ã‚’æ¢ã™ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = r"C:\Users\progr\Desktop\Python\mercari_dorekai\downloads"
PRODUCT_DATA_GLOB = os.path.join(DATA_DIR, "product_data_*.csv")
SCROLL_TIMES = 10  # ä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ï¼ˆå•†å“ãƒšãƒ¼ã‚¸å†…ã®ç”»åƒå–å¾—ã®ãŸã‚ï¼‰
WAIT_BETWEEN_IMAGES = 0.3  # ç§’
# =================================

os.makedirs(IMAGE_DIR, exist_ok=True)
os.makedirs(BROKEN_IMAGE_DIR, exist_ok=True)

# åœ¨åº«ãªã—ç”¨ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆç”»åƒã‚’ç§»å‹•ã™ã‚‹å…ˆï¼‰
NO_STOCK_DIR = os.path.join(IMAGE_DIR, 'åœ¨åº«ãªã—')
os.makedirs(NO_STOCK_DIR, exist_ok=True)

# å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå‰Šé™¤å‡¦ç†æ¸ˆã¿ã®è­˜åˆ¥å­ã‚’è¨˜éŒ²ï¼‰
PROCESSED_DELETIONS_FILE = os.path.join(os.path.dirname(__file__), 'processed_deleted_images.txt')

def load_processed_deleted_ids():
    ids = set()
    try:
        with open(PROCESSED_DELETIONS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                v = line.strip()
                if v:
                    ids.add(v)
    except FileNotFoundError:
        pass
    return ids

def append_processed_deleted_ids(ids):
    try:
        with open(PROCESSED_DELETIONS_FILE, 'a', encoding='utf-8') as f:
            for v in ids:
                if v:
                    f.write(v + '\n')
    except Exception as e:
        print(f"âš ï¸ å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def check_broken_images():
    """ç ´æãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å“ç•ªã‚’æŠ½å‡ºã—ã€æ—¢å­˜ç”»åƒã‚’å‰Šé™¤"""
    broken_product_ids = set()
    
    if not os.path.exists(BROKEN_IMAGE_DIR):
        print(f"âš ï¸ ç ´æãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {BROKEN_IMAGE_DIR}")
        return broken_product_ids
    
    try:
        files = os.listdir(BROKEN_IMAGE_DIR)
        if not files:
            print(f"âœ… ç ´æãƒ•ã‚©ãƒ«ãƒ€ã¯ç©ºã§ã™")
            return broken_product_ids
        
        print(f"\nğŸ” ç ´æãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒã‚§ãƒƒã‚¯ä¸­: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        
        for filename in files:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å“ç•ªã‚’æŠ½å‡ºï¼ˆproduct_id-*.jpg å½¢å¼ã‚’æƒ³å®šï¼‰
            match = re.match(r'^(.*?)-', filename)
            if match:
                product_id = match.group(1)
                broken_product_ids.add(product_id)
        
        if broken_product_ids:
            print(f"ğŸ”§ ç ´æç”»åƒæ¤œå‡º: {len(broken_product_ids)} å“ç•ª")
            
            # æ—¢å­˜ç”»åƒã‚’å‰Šé™¤
            for product_id in broken_product_ids:
                print(f"\n  ğŸ—‘ï¸ å“ç•ª {product_id} ã®æ—¢å­˜ç”»åƒã‚’å‰Šé™¤ä¸­...")
                deleted_count = delete_product_images(product_id)
                if deleted_count > 0:
                    print(f"    å‰Šé™¤å®Œäº†: {deleted_count} ãƒ•ã‚¡ã‚¤ãƒ«")
        
        return broken_product_ids
        
    except Exception as e:
        print(f"âŒ ç ´æãƒ•ã‚©ãƒ«ãƒ€ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return broken_product_ids

def cleanup_broken_folder(product_id):
    """å†å–å¾—æˆåŠŸå¾Œã€ç ´æãƒ•ã‚©ãƒ«ãƒ€ã®è©²å½“ç”»åƒã‚’å‰Šé™¤"""
    try:
        deleted_count = 0
        for filename in os.listdir(BROKEN_IMAGE_DIR):
            if filename.startswith(f"{product_id}-"):
                file_path = os.path.join(BROKEN_IMAGE_DIR, filename)
                try:
                    os.remove(file_path)
                    print(f"  ğŸ§¹ ç ´æãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å‰Šé™¤: {filename}")
                    deleted_count += 1
                except Exception as e:
                    print(f"  âš ï¸ å‰Šé™¤å¤±æ•—: {filename} - {e}")
        return deleted_count
    except Exception as e:
        print(f"  âš ï¸ ç ´æãƒ•ã‚©ãƒ«ãƒ€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def get_extension_from_content_type(content_type):
    """Content-Type æ–‡å­—åˆ—ã‹ã‚‰æ‹¡å¼µå­ã‚’æ¨æ¸¬ã™ã‚‹"""
    mapping = {"image/jpeg": ".jpg", "image/png": ".png", "image/gif": ".gif", "image/webp": ".webp"}
    return mapping.get(content_type.split(";")[0].strip().lower(), ".jpg")

def safe_filename(url):
    return os.path.basename(urlparse(url).path)

def download_image(url, path):
    if os.path.exists(path):
        print(f"  â© æ—¢ã«ä¿å­˜æ¸ˆã¿: {path}")
        return
    try:
        # ã¾ãš HEAD ã§ Content-Type ã‚’ç¢ºèªï¼ˆãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚„éç”»åƒã‚’é™¤å¤–ï¼‰
        try:
            head = requests.head(url, allow_redirects=True, timeout=5)
        except Exception:
            head = None

        if head:
            if head.status_code != 200:
                print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚­ãƒƒãƒ—: HEAD returned {head.status_code} - {url}")
                return
            ctype = head.headers.get("Content-Type", "")
            if not ctype.startswith("image/"):
                print(f"  â© éç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {ctype} - {url}")
                return
            # æ‹¡å¼µå­ãŒãªã‘ã‚Œã° Content-Type ã‹ã‚‰æ±ºå®š
            if not os.path.splitext(path)[1]:
                ext = get_extension_from_content_type(ctype)
                path = path + ext if not path.endswith(ext) else path

        # å®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGETï¼‰
        r = requests.get(url, stream=True, timeout=15)
        if r.status_code == 200 and r.headers.get("Content-Type", "").startswith("image/"):
            # æ‹¡å¼µå­ãŒã¾ã ä»˜ã„ã¦ã„ãªã‘ã‚Œã° Content-Type ã‹ã‚‰å†æ±ºå®š
            if not os.path.splitext(path)[1]:
                ctype = r.headers.get("Content-Type", "")
                ext = get_extension_from_content_type(ctype)
                path += ext
            with open(path, "wb") as f:
                for chunk in r.iter_content(1024):
                    if chunk:
                        f.write(chunk)
            print(f"  âœ… ä¿å­˜å®Œäº†: {path}")
            time.sleep(WAIT_BETWEEN_IMAGES)
        else:
            status = r.status_code if r is not None else "N/A"
            ctype = r.headers.get("Content-Type", "") if r is not None else ""
            print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: HTTP {status} / {ctype} - {url}")
    except Exception as e:
        print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e} - {url}")

def find_latest_product_csv():
    files = glob.glob(PRODUCT_DATA_GLOB)
    if not files:
        return None
    latest = max(files, key=os.path.getmtime)
    return latest

def read_product_ids_from_csv(csv_path):
    """
    CSV ã‹ã‚‰ product_id ã‚’èª­ã¿ã€å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ãŒ '1' ã¾ãŸã¯ '2' ã®è¡Œã‚’è¿”ã™ã€‚
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ '2': ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ID
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ '1': ç”»åƒå‰Šé™¤å¯¾è±¡ã®ID
    - ãƒ˜ãƒƒãƒ€åã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã¦æ¤œå‡ºçŠ¶æ³ã‚’ç¢ºèª
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å€¤ã¯å…¨è§’æ•°å­—ã‚„å°æ•°è¡¨è¨˜ç­‰ã‚’æ­£è¦åŒ–ã—ã¦åˆ¤å®š
    - è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
    - å•†å“åã¨å•†å“èª¬æ˜ã®æœ€åˆã®æ•°å­—ãŒä¸€è‡´ã™ã‚‹å ´åˆã®ã¿ã€ãã®æ•°å­—ã‚‚è¿”ã™
    æˆ»ã‚Šå€¤: (status_2_data, status_1_ids) ã®ã‚¿ãƒ—ãƒ«
              status_2_data ã¯ (product_id, matched_number) ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    product_data_status_2 = []  # (product_id, matched_number) ã®ãƒªã‚¹ãƒˆ
    product_ids_status_1 = []
    encodings_to_try = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis', 'euc_jp', 'iso2022_jp', 'latin-1']
    last_exc = None

    def normalize_digits(s):
        if s is None:
            return ""
        s = str(s).strip().strip('"').strip("'")
        trans = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
        return s.translate(trans)

    for enc in encodings_to_try:
        try:
            with open(csv_path, newline='', encoding=enc) as f:
                reader = csv.DictReader(f)
                fieldnames = reader.fieldnames

                print(f"[debug] try encoding={enc}, detected fieldnames={fieldnames}")

                def guess_id_field(fns):
                    for fn in fns:
                        if re.search(r'product.*id|product_id|å•†å“.*id|å•†å“id|^id$', fn or '', re.I):
                            return fn
                    for fn in fns:
                        if re.search(r'\bid\b', fn or '', re.I):
                            return fn
                    return fns[0] if fns else None

                # ã“ã“ã‚’ä¿®æ­£ï¼šã¾ãšã€Œå•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€ã‚’å„ªå…ˆã§æ¢ã™ï¼ˆä»¥å‰ã¯ã€ŒçŠ¶æ…‹ã€ã‚’å…ˆã«å–ã£ã¦ã—ã¾ã£ã¦ã„ãŸï¼‰
                def guess_status_field(fns):
                    for fn in fns:
                        if re.search(r'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|product.*status|product_status|^status$', fn or '', re.I):
                            return fn
                    for fn in fns:
                        if re.search(r'å•†å“.*ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|status', fn or '', re.I):
                            return fn
                    for fn in fns:
                        if re.search(r'çŠ¶æ…‹', fn or '', re.I):
                            return fn
                    return None

                def guess_name_field(fns):
                    for fn in fns:
                        if re.search(r'å•†å“å|product.*name|product_name|^name$|^å•†å“$', fn or '', re.I):
                            return fn
                    return None

                def guess_description_field(fns):
                    for fn in fns:
                        if re.search(r'å•†å“èª¬æ˜|product.*description|description|èª¬æ˜', fn or '', re.I):
                            return fn
                    return None

                def guess_sku1_field(fns):
                    # SKU1 ã‚„åœ¨åº«æ•°ã‚’è¡¨ã™åˆ—åã‚’å„ªå…ˆçš„ã«æ¢ã™
                    patterns = [r'SKU1_ç¾åœ¨ã®åœ¨åº«æ•°', r'SKU1', r'sku1', r'SKU1åœ¨åº«', r'SKU1.*åœ¨åº«', r'SKU.*åœ¨åº«', r'ç¾åœ¨ã®åœ¨åº«æ•°', r'åœ¨åº«æ•°', r'åœ¨åº«']
                    for pat in patterns:
                        for fn in fns:
                            if re.search(pat, fn or '', re.I):
                                return fn
                    return None

                status_counts = {}

                if not fieldnames:
                    f.seek(0)
                    rows = [r for r in csv.reader(f) if any(cell.strip() for cell in r)]
                    if not rows:
                        product_ids = []
                        break
                    header = rows[0]
                    data_rows = rows[1:]
                    id_idx = None
                    status_idx = None
                    for i, hn in enumerate(header):
                        if re.search(r'product.*id|product_id|å•†å“.*id|å•†å“id|^id$', hn or '', re.I):
                            id_idx = i
                            break
                    if id_idx is None:
                        for i, hn in enumerate(header):
                            if re.search(r'\bid\b', hn or '', re.I):
                                id_idx = i
                                break
                    if id_idx is None:
                        id_idx = 0

                    for i, hn in enumerate(header):
                        if re.search(r'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹|product.*status|status|çŠ¶æ…‹', hn or '', re.I):
                            status_idx = i
                            break

                    for row in data_rows:
                        if len(row) <= id_idx:
                            continue
                        pid = row[id_idx].strip()
                        if not pid:
                            continue
                        status_val = None
                        if status_idx is not None and len(row) > status_idx:
                            status_val = normalize_digits(row[status_idx])
                        key = status_val or "(empty)"
                        status_counts[key] = status_counts.get(key, 0) + 1
                        m = re.search(r'(\d+)', status_val or "")
                        if m:
                            status_num = int(m.group(1))
                            if status_num == 2:
                                product_data_status_2.append((pid, None))
                            elif status_num == 1:
                                product_ids_status_1.append((pid, None))

                else:
                    id_field = guess_id_field(fieldnames)
                    status_field = guess_status_field(fieldnames)
                    name_field = guess_name_field(fieldnames)
                    desc_field = guess_description_field(fieldnames)

                    print(f"[debug] guessed id_field={id_field}, status_field={status_field}, name_field={name_field}, desc_field={desc_field}")

                    if not id_field:
                        id_field = fieldnames[0]

                    if not status_field:
                        print("CSV ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†å¯¾è±¡ãªã—ã¨ã—ã¾ã™ã€‚")
                        return ([], [])

                    sku1_field = guess_sku1_field(fieldnames)

                    for row in reader:
                        pid = (row.get(id_field) or "").strip()
                        if not pid:
                            continue

                        raw_status = row.get(status_field)
                        status_val = normalize_digits(raw_status)
                        key = status_val or "(empty)"
                        status_counts[key] = status_counts.get(key, 0) + 1

                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ•°å€¤ã‚’å–ã‚Šå‡ºã™ï¼ˆç„¡ã‘ã‚Œã° Noneï¼‰
                        status_num = None
                        m = re.search(r'(\d+)', status_val)
                        if m:
                            try:
                                status_num = int(m.group(1))
                            except Exception:
                                status_num = None

                        # SKU1 ã®åœ¨åº«æ•°ã‚’ç¢ºèªï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° None ã¨æ‰±ã†ï¼‰
                        stock_num = None
                        if sku1_field:
                            raw_stock = row.get(sku1_field)
                            stock_val = normalize_digits(raw_stock)
                            sm = re.search(r'(\d+)', stock_val)
                            if sm:
                                try:
                                    stock_num = int(sm.group(1))
                                except Exception:
                                    stock_num = None

                        # åˆ¤å®š: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ2ã‹ã¤ SKU1 åœ¨åº«ãŒ 0 ã§ãªã„å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡
                        is_active = (status_num == 2)
                        is_sold_or_deleted = (not is_active) or (stock_num == 0)

                        matched_number = None
                        if name_field and desc_field:
                            product_name = (row.get(name_field) or "").strip()
                            product_desc = (row.get(desc_field) or "").strip()
                            name_match = re.search(r'^(\d+)', product_name)
                            desc_match = re.search(r'^(\d+)', product_desc)
                            if name_match and desc_match:
                                name_number = name_match.group(1)
                                desc_number = desc_match.group(1)
                                if name_number == desc_number:
                                    matched_number = name_number
                                    print(f"  âœ“ å“ç•ªä¸€è‡´: {pid} -> {matched_number}")

                        if is_sold_or_deleted:
                            product_ids_status_1.append((pid, matched_number))
                            continue

                        # ã“ã“ã¾ã§æ¥ãŸã‚‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹2ã‹ã¤åœ¨åº«ãŒå­˜åœ¨ã™ã‚‹ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ï¼‰
                        product_data_status_2.append((pid, matched_number))

                print(f"[debug] encoding={enc} status_counts (sample up to 20 keys) = {dict(list(status_counts.items())[:20])}")
                print(f"CSV èª­ã¿è¾¼ã¿æˆåŠŸ: encoding={enc}, ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹2={len(product_data_status_2)}, ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹1={len(product_ids_status_1)}")
                # é‡è¤‡ã‚’å‰Šé™¤ï¼ˆæœ€åˆã®å‡ºç¾ã‚’ä¿æŒï¼‰
                seen_ids = set()
                unique_status_2 = []
                for pid, num in product_data_status_2:
                    if pid not in seen_ids:
                        seen_ids.add(pid)
                        unique_status_2.append((pid, num))

                seen_ids_1 = set()
                unique_status_1 = []
                for pid, num in product_ids_status_1:
                    if pid not in seen_ids_1:
                        seen_ids_1.add(pid)
                        unique_status_1.append((pid, num))

                return (unique_status_2, unique_status_1)

        except Exception as e:
            last_exc = e
            continue

    print(f"CSV èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {last_exc}")
    return ([], [])

def get_downloaded_product_ids():
    """IMAGE_DIR å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã® product_id ã¾ãŸã¯ matched_number ã®ã‚»ãƒƒãƒˆã‚’è¿”ã™"""
    downloaded_ids = set()
    try:
        for filename in os.listdir(IMAGE_DIR):
            # ãƒ•ã‚¡ã‚¤ãƒ«åãŒ "number-*" ã®å½¢å¼ã§ã‚ã‚‹ã¨ä»®å®š
            match = re.match(r'^(.*?)-', filename)
            if match:
                downloaded_ids.add(match.group(1))
    except FileNotFoundError:
        print(f"Warning: Image directory not found: {IMAGE_DIR}")
    return downloaded_ids

def delete_product_images(identifier):
    """æŒ‡å®šã•ã‚ŒãŸidentifierï¼ˆproduct_idã¾ãŸã¯matched_numberï¼‰ã®ç”»åƒã‚’å…¨ã¦å‰Šé™¤ã™ã‚‹"""
    deleted_count = 0
    try:
        for filename in os.listdir(IMAGE_DIR):
            # ãƒ•ã‚¡ã‚¤ãƒ«åãŒ "identifier-*" ã®å½¢å¼ã§ã‚ã‚‹ã‹ç¢ºèª
            if filename.startswith(f"{identifier}-"):
                file_path = os.path.join(IMAGE_DIR, filename)
                try:
                    os.remove(file_path)
                    print(f"  ğŸ—‘ï¸ å‰Šé™¤: {filename}")
                    deleted_count += 1
                except Exception as e:
                    print(f"  âŒ å‰Šé™¤å¤±æ•—: {filename} - {e}")
    except FileNotFoundError:
        pass
    return deleted_count

def move_product_images(identifier):
    """æŒ‡å®šã•ã‚ŒãŸidentifierï¼ˆproduct_idã¾ãŸã¯matched_numberï¼‰ã®ç”»åƒã‚’åœ¨åº«ãªã—ãƒ•ã‚©ãƒ«ãƒ€ã¸ç§»å‹•ã™ã‚‹"""
    moved_count = 0
    try:
        for filename in os.listdir(IMAGE_DIR):
            if filename.startswith(f"{identifier}-"):
                src = os.path.join(IMAGE_DIR, filename)
                dst = os.path.join(NO_STOCK_DIR, filename)
                try:
                    shutil.move(src, dst)
                    print(f"  ğŸ“¦ ç§»å‹•: {filename} -> åœ¨åº«ãªã—ãƒ•ã‚©ãƒ«ãƒ€")
                    moved_count += 1
                except Exception as e:
                    print(f"  âŒ ç§»å‹•å¤±æ•—: {filename} - {e}")
    except FileNotFoundError:
        pass
    return moved_count

def process_product_pages(product_data, downloaded_ids, is_broken_retry=False):
    """
    product_data: (product_id, matched_number) ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
                  matched_number ãŒ None ã®å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    """
    if not product_data:
        print("å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        for product_id, matched_number in product_data:
            if not product_id:
                continue
            
            # matched_number ãŒ None ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå•†å“åã¨å•†å“èª¬æ˜ã®æ•°å­—ãŒä¸€è‡´ã—ãªã„ï¼‰
            if matched_number is None and not is_broken_retry:
                print(f"\nâ–¶ {product_id} ã¯å“ç•ªä¸ä¸€è‡´ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ã™ã‚‹è­˜åˆ¥å­ã‚’æ±ºå®š
            file_identifier = matched_number if matched_number else product_id
            
            # ç ´æç”»åƒã®å†å–å¾—ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯
            if not is_broken_retry and file_identifier in downloaded_ids:
                print(f"\nâ–¶ {file_identifier} ã¯æ—¢ã«ç”»åƒä¿å­˜æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            product_url = f"https://jp.mercari.com/shops/product/{product_id}"
            print(f"\nâ–¶ {product_id} (ä¿å­˜å: {file_identifier}) ã‚’å‡¦ç†ä¸­... ({product_url})")

            try:
                detail_page = browser.new_page()
                detail_page.goto(product_url, timeout=60000)
                # å¿…è¦ãªã‚‰ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆç”»åƒãŒé…å»¶èª­ã¿è¾¼ã¿ã•ã‚Œã‚‹å ´åˆã«å‚™ãˆï¼‰
                prev_height = 0
                for _ in range(SCROLL_TIMES):
                    detail_page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    detail_page.wait_for_timeout(1000)
                    height = detail_page.evaluate("document.body.scrollHeight")
                    if height == prev_height:
                        break
                    prev_height = height

                # ç”»åƒURLæŠ½å‡º
                img_elements = detail_page.query_selector_all('div[data-testid="carousel-item"] picture img')
                if not img_elements:
                    # åˆ¥ã‚»ãƒ¬ã‚¯ã‚¿ã®å¯èƒ½æ€§ã«ã‚‚å¯¾å¿œ
                    img_elements = detail_page.query_selector_all('img')
                if not img_elements:
                    print("  âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    print(f"  ğŸ–¼ï¸ {len(img_elements)}æšã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                    download_success = False
                    for i, img_element in enumerate(img_elements):
                        img_url = img_element.get_attribute('src')
                        if img_url:
                            # æ‹¡å¼µå­ã¯ .jpg å›ºå®šã«ã›ãšå…ƒURLã‹ã‚‰å–å¾—ã™ã‚‹
                            fname = safe_filename(img_url)
                            # URLã«'@'ãŒå«ã¾ã‚Œã‚‹å ´åˆã‚’è€ƒæ…®ã—ã¦ã€'@'ä»¥é™ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—
                            fname_main = fname.split('@')[0]
                            ext = os.path.splitext(fname_main)[1] or ".jpg"
                            # file_identifierã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ
                            file_name = f"{file_identifier}-{i+1}{ext}"
                            file_path = os.path.join(IMAGE_DIR, file_name)
                            download_image(img_url, file_path)
                            download_success = True
                        else:
                            print("  âŒ ç”»åƒURLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    
                    # ç ´æç”»åƒã®å†å–å¾—ã§æˆåŠŸã—ãŸå ´åˆã€ç ´æãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    if is_broken_retry and download_success:
                        cleanup_broken_folder(file_identifier)
                        
                detail_page.close()
            except Exception as e:
                print(f"  âŒ ãƒšãƒ¼ã‚¸å‡¦ç†å¤±æ•— ({product_id}): {e}")

        browser.close()

def main():
    # 1. ç ´æãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå„ªå…ˆå‡¦ç†ï¼‰
    broken_product_ids = check_broken_images()
    
    # 2. CSVã‚’èª­ã¿è¾¼ã¿
    latest_csv = find_latest_product_csv()
    if not latest_csv:
        print(f"product_data_*.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {PRODUCT_DATA_GLOB}")
        return
    print(f"æœ€æ–°ã® CSV: {latest_csv}")
    product_data_status_2, product_ids_status_1 = read_product_ids_from_csv(latest_csv)
    print(f"CSV ã‹ã‚‰å–å¾—ã—ãŸå‡¦ç†å¯¾è±¡ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹2): {len(product_data_status_2)} ä»¶")
    print(f"CSV ã‹ã‚‰å–å¾—ã—ãŸå‰Šé™¤å¯¾è±¡ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹1): {len(product_ids_status_1)} ä»¶")
    
    # å“ç•ªä¸€è‡´ã—ãŸä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    matched_count = sum(1 for _, num in product_data_status_2 if num is not None)
    print(f"  ã†ã¡å“ç•ªä¸€è‡´: {matched_count} ä»¶")
    
    # 3. ç ´æç”»åƒã®å†å–å¾—ï¼ˆå„ªå…ˆï¼‰
    if broken_product_ids:
        print(f"\nğŸ”§ ç ´æç”»åƒã®å†å–å¾—ã‚’å„ªå…ˆçš„ã«å®Ÿè¡Œ: {len(broken_product_ids)} å“ç•ª")
        # ç ´æç”»åƒã®å†å–å¾—ã¯ product_id ã‚’ãã®ã¾ã¾ä½¿ç”¨
        broken_data = [(pid, None) for pid in broken_product_ids]
        process_product_pages(broken_data, set(), is_broken_retry=True)
    
    # 4. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹1ã®å•†å“ç”»åƒã‚’å‰Šé™¤ï¼ˆå±¥æ­´å‚ç…§ã—ã¦2åº¦å‰Šé™¤ã—ãªã„ï¼‰
    if product_ids_status_1:
        print("\nâ–¶ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹1ã®å•†å“ç”»åƒã‚’å‰Šé™¤ä¸­... (å±¥æ­´ã‚’å‚ç…§ã—ã¾ã™)")
        total_deleted = 0
        processed_deleted = load_processed_deleted_ids()
        print(f"  å±¥æ­´ç™»éŒ²æ¸ˆã¿è­˜åˆ¥å­: {len(processed_deleted)} ä»¶")
        for product_id, matched_number in product_ids_status_1:
            if not product_id:
                continue

            # å±¥æ­´ã«ç™»éŒ²æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            if product_id in processed_deleted or (matched_number and matched_number in processed_deleted):
                print(f"\nâ–¶ {product_id} ã¯å±¥æ­´ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            print(f"\nâ–¶ {product_id} ã®ç”»åƒã‚’åœ¨åº«ãªã—ãƒ•ã‚©ãƒ«ãƒ€ã¸ç§»å‹•ä¸­...")
            deleted_cnt = 0
            d1 = move_product_images(product_id)
            deleted_cnt += d1
            # matched_number ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚‚ç§»å‹•ã‚’è©¦ã¿ã‚‹
            d2 = 0
            if matched_number:
                d2 = move_product_images(matched_number)
                deleted_cnt += d2

            if deleted_cnt > 0:
                print(f"  âœ… {deleted_cnt}æšã®ç”»åƒã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            else:
                print(f"  â„¹ï¸ å‰Šé™¤å¯¾è±¡ã®ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            total_deleted += deleted_cnt

            # å±¥æ­´ã«è¿½è¨˜ï¼ˆå‰Šé™¤ãŒç„¡ãã¦ã‚‚å‡¦ç†æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²ã—ã¦æ¬¡å›ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ï¼‰
            ids_to_append = [product_id]
            if matched_number:
                ids_to_append.append(matched_number)
            append_processed_deleted_ids(ids_to_append)
            processed_deleted.update(ids_to_append)

        print(f"\nâœ… åˆè¨ˆ {total_deleted}æšã®ç”»åƒã‚’å‰Šé™¤ã—ã¾ã—ãŸ\n")
    
    # 5. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹2ã®å•†å“ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    downloaded_ids = get_downloaded_product_ids()
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«è­˜åˆ¥å­æ•°: {len(downloaded_ids)}")
    process_product_pages(product_data_status_2, downloaded_ids, is_broken_retry=False)
    print("\nâœ… å‡¦ç†å®Œäº†")

if __name__ == "__main__":
    main()
