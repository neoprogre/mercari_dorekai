import os
import csv
import time
import sys
import re
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

try:
    # æ—¢å­˜ã®ãƒ‰ãƒ©ã‚¤ãƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–¢æ•°ã‚’å†åˆ©ç”¨
    from yahooku_dorekai import setup_driver
except ImportError:
    print("yahooku_dorekai.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

# --- è¨­å®š ---
# å‡ºå“ä¸­
SELLING_URL = "https://auctions.yahoo.co.jp/my/selling"
# è½æœ­è€…ãªã—ï¼ˆçµ‚äº†åˆ†ï¼‰
CLOSED_URL = "https://auctions.yahoo.co.jp/my/closed?hasWinner=0"
# å‡ºåŠ›ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«å
OUTPUT_CSV = os.path.join(os.path.dirname(os.path.abspath(__file__)), "products_yahooku.csv")
# å†å‡ºå“å‡¦ç†æ¸ˆã¿ãƒ­ã‚°
PROCESSED_RELIST_LOG = os.path.join(os.path.dirname(os.path.abspath(__file__)), "processed_relist_ids.txt")
# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå‡ºå“æ•°ã®ä¸Šé™
MAX_ACTIVE_ITEMS = 100

def log(msg):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹"""
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

def get_management_info(driver, url):
    """å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ï¼ˆç®¡ç†ç”»é¢ï¼‰ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹æ•°ãƒ»ã‚¦ã‚©ãƒƒãƒæ•°ãƒ»å…¥æœ­æ•°ã‚’å–å¾—"""
    info = {'access': '0', 'watch': '0', 'bids': '0'}
    if not url:
        return info
        
    original_window = driver.current_window_handle
    
    try:
        # æ–°ã—ã„ã‚¿ãƒ–ã‚’é–‹ã
        driver.switch_to.new_window('tab')
        driver.get(url)
        time.sleep(1.5) # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…ã¡

        # ç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ (id="management") å†…ã®æƒ…å ±ã‚’å–å¾—
        # ã‚¢ã‚¯ã‚»ã‚¹æ•°
        try:
            elem = driver.find_element(By.XPATH, "//*[@id='management']//p[contains(text(), 'ã‚¢ã‚¯ã‚»ã‚¹')]/ancestor::li[1]//span")
            info['access'] = elem.text.strip()
        except NoSuchElementException:
            pass

        # ã‚¦ã‚©ãƒƒãƒæ•°
        try:
            elem = driver.find_element(By.XPATH, "//*[@id='management']//p[contains(text(), 'ã‚¦ã‚©ãƒƒãƒ')]/ancestor::li[1]//span")
            info['watch'] = elem.text.strip()
        except NoSuchElementException:
            pass

        # å…¥æœ­æ•°
        try:
            elem = driver.find_element(By.XPATH, "//*[@id='management']//p[contains(text(), 'å…¥æœ­')]/ancestor::li[1]//span")
            info['bids'] = elem.text.strip()
        except NoSuchElementException:
            pass

    except Exception as e:
        log(f"  âŒ è©³ç´°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        # ã‚¿ãƒ–ã‚’é–‰ã˜ã¦å…ƒã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«æˆ»ã‚‹
        try:
            if len(driver.window_handles) > 1:
                driver.close()
            driver.switch_to.window(original_window)
        except Exception:
            pass
        
    return info

def scrape_page_items(driver, status_label):
    """ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰å•†å“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
    page_items = []
    
    # ãƒã‚¤ãƒ»ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ã‚»ãƒ¬ã‚¯ã‚¿æˆ¦ç•¥
    # div#itm ä»¥ä¸‹ã® li è¦ç´ ã‚’å–å¾—
    try:
        # å•†å“ãƒªã‚¹ãƒˆã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
        product_elements = driver.find_elements(By.CSS_SELECTOR, "#itm ul > li")
        
        if not product_elements:
            log("âš ï¸ å•†å“ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (#itm ul > li)ã€‚")
            return []

        log(f"ğŸ“Š {len(product_elements)} ä»¶ã®å•†å“è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

        for elem in product_elements:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨URL
                # data-cl-params ã« _cl_link:tc ãŒå«ã¾ã‚Œã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                title_elem = elem.find_element(By.CSS_SELECTOR, "a[data-cl-params*='_cl_link:tc']")
                title = title_elem.text.strip()
                url = title_elem.get_attribute('href')
                
                # ç”»åƒ
                # data-cl-params ã« _cl_link:ic ãŒå«ã¾ã‚Œã‚‹ãƒªãƒ³ã‚¯å†…ã® img
                try:
                    img_elem = elem.find_element(By.CSS_SELECTOR, "a[data-cl-params*='_cl_link:ic'] img")
                    image_url = img_elem.get_attribute('src')
                except NoSuchElementException:
                    image_url = ""

                # ä¾¡æ ¼
                # å††ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
                price = "0"
                try:
                    text_content = elem.text
                    price_match = re.search(r'([\d,]+)å††', text_content)
                    if price_match:
                        price = price_match.group(1).replace(',', '')
                except Exception:
                    pass

                # æ®‹ã‚Šæ™‚é–“
                time_left = ""
                try:
                    time_elem = elem.find_element(By.CSS_SELECTOR, "svg[aria-label='æ®‹ã‚Šæ™‚é–“'] + span")
                    time_left = time_elem.text.strip()
                except NoSuchElementException:
                    pass

                # URLã‹ã‚‰ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDã‚’æŠ½å‡º
                auction_id = ""
                if url:
                    match = re.search(r'/auction/([a-zA-Z0-9]+)', url)
                    if match:
                        auction_id = match.group(1)

                if auction_id:
                    page_items.append({
                        'auction_id': auction_id,
                        'title': title,
                        'price': price,
                        'url': url,
                        'status': status_label,
                        'time_left': time_left,
                    })
            except NoSuchElementException:
                continue
            except Exception as e:
                log(f"âŒ å•†å“æƒ…å ±æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # å„å•†å“ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—
        for item in page_items:
            log(f"  ğŸ” è©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­: {item['title'][:20]}...")
            mgmt_info = get_management_info(driver, item['url'])
            item.update(mgmt_info)
            time.sleep(1) # è² è·è»½æ¸›

    except Exception as e:
        log(f"âŒ ãƒšãƒ¼ã‚¸è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    
    return page_items

def scrape_url(driver, start_url, status_label):
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã—ãªãŒã‚‰å…¨å•†å“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    all_items = []
    current_url = start_url
    
    while True:
        log(f"ãƒšãƒ¼ã‚¸ç§»å‹•: {current_url}")
        driver.get(current_url)
        time.sleep(3) # èª­ã¿è¾¼ã¿å¾…æ©Ÿ

        # --- ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç† ---
        if "login.yahoo.co.jp" in driver.current_url:
            log("ğŸ”’ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ­ã‚°ã‚¤ãƒ³ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚")
            input("ğŸ‘‰ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€ã“ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„: ")
            log("âœ… ãƒ­ã‚°ã‚¤ãƒ³ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’å†é–‹ã—ã¾ã™ã€‚")
            driver.get(current_url)
            time.sleep(3)

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦èª­ã¿è¾¼ã¿
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º
        items = scrape_page_items(driver, status_label)
        if items:
            all_items.extend(items)
            log(f"  -> {len(items)} ä»¶å–å¾— (åˆè¨ˆ: {len(all_items)} ä»¶)")
        else:
            log("  -> å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # --- æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ç¢ºèª ---
        try:
            # "æ¬¡ã¸" ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            next_link = None
            links = driver.find_elements(By.TAG_NAME, "a")
            for link in links:
                if "æ¬¡ã¸" in link.text:
                    next_link = link
                    break
            
            if next_link:
                current_url = next_link.get_attribute("href")
            else:
                log("æ¬¡ã®ãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                break
        except Exception as e:
            log(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            break

    return all_items

def load_processed_ids(log_file):
    """å‡¦ç†æ¸ˆã¿ã®IDã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(log_file):
        return set()
    with open(log_file, "r", encoding="utf-8") as f:
        return {line.strip() for line in f}

def save_processed_id(auction_id, log_file):
    """å‡¦ç†æ¸ˆã¿ã®IDã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã™ã‚‹"""
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"{auction_id}\n")

def relist_item(driver, auction_id):
    """æŒ‡å®šã•ã‚ŒãŸã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDã®å•†å“ã‚’å†å‡ºå“ã™ã‚‹"""
    relist_url = f"https://auctions.yahoo.co.jp/sell/jp/show/resubmit?aID={auction_id}"
    log(f"  å†å‡ºå“ãƒšãƒ¼ã‚¸ã«ç§»å‹•: {relist_url}")
    driver.get(relist_url)
    time.sleep(4) # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã¨JSã®å®Ÿè¡Œã‚’å¾…ã¤

    try:
        # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.NAME, "Title"))
        )
        log("  å†å‡ºå“ãƒ•ã‚©ãƒ¼ãƒ ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚")

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã•ã›ã‚‹
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)

        # ã€Œç¢ºèªç”»é¢ã¸ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        confirm_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.ID, "submit_form_btn"))
        )
        confirm_button.click()
        log("  âœ… ç¢ºèªç”»é¢ã¸é€²ã‚€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã—ãŸã€‚")

        # ã€Œå‡ºå“ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        final_submit_button = WebDriverWait(driver, 20).until(
            EC.element_to_be_clickable((By.ID, "auc_preview_submit_up"))
        )
        final_submit_button.click()
        log("  âœ… å‡ºå“ã™ã‚‹ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã—ãŸã€‚")

        # å®Œäº†ã‚’å¾…ã¤ (å‡ºå“å®Œäº†ãƒšãƒ¼ã‚¸ or å‡ºå“ä¸­ãƒªã‚¹ãƒˆã«é·ç§»ã™ã‚‹ã®ã‚’å¾…ã¤)
        WebDriverWait(driver, 30).until(
            EC.any_of(
                EC.url_contains("show/complete"),
                EC.url_contains("my/selling")
            )
        )
        log(f"  âœ… {auction_id} ã®å†å‡ºå“ãŒå®Œäº†ã—ãŸã‚ˆã†ã§ã™ã€‚")
        return True

    except Exception as e:
        log(f"  âŒ å†å‡ºå“å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        error_dir = "error_artifacts"
        os.makedirs(error_dir, exist_ok=True)
        ss_path = os.path.join(error_dir, f"relist_error_{auction_id}_{int(time.time())}.png")
        driver.save_screenshot(ss_path)
        log(f"  ğŸ“¸ ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {ss_path}")
        return False

def save_to_csv(items):
    """
    å–å¾—ã—ãŸå•†å“æƒ…å ±ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
    """
    if not items:
        log("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # é‡è¤‡é™¤å» (auction_id)
    unique_items = {}
    for item in items:
        unique_items[item['auction_id']] = item
    
    items = list(unique_items.values())

    log(f"æŠ½å‡ºã—ãŸ {len(items)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {OUTPUT_CSV} ã«ä¿å­˜ã—ã¾ã™...")
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as f:
        fieldnames = ['auction_id', 'status', 'title', 'price', 'bids', 'watch', 'access', 'time_left']
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        
        writer.writeheader()
        writer.writerows(items)
    log(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {OUTPUT_CSV}")

def relist_if_needed(driver, all_items):
    log("\n--- [3/3] è‡ªå‹•å†å‡ºå“ãƒã‚§ãƒƒã‚¯ ---")

    if not all_items:
        log("å•†å“ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€å†å‡ºå“å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    df = pd.DataFrame(all_items)
    
    # æ•°å€¤ã«å¤‰æ›
    for col in ['bids', 'watch', 'access', 'price']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

    active_items = df[df['status'] == 'å‡ºå“ä¸­']
    ended_items = df[df['status'] == 'çµ‚äº†ï¼ˆè½æœ­è€…ãªã—ï¼‰'].copy()

    log(f"ç¾åœ¨ã®å‡ºå“æ•°: {len(active_items)}ä»¶")
    if len(active_items) >= MAX_ACTIVE_ITEMS:
        log(f"âœ… å‡ºå“æ•°ãŒä¸Šé™({MAX_ACTIVE_ITEMS}ä»¶)ã«é”ã—ã¦ã„ã‚‹ãŸã‚ã€å†å‡ºå“ã¯è¡Œã„ã¾ã›ã‚“ã€‚")
        return

    num_to_relist = MAX_ACTIVE_ITEMS - len(active_items)
    log(f"â„¹ï¸ {num_to_relist} ä»¶ã®å†å‡ºå“æ ãŒã‚ã‚Šã¾ã™ã€‚")

    processed_ids = load_processed_ids(PROCESSED_RELIST_LOG)
    ended_items = ended_items[~ended_items['auction_id'].isin(processed_ids)]
    
    # ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯: å‡ºå“ä¸­ã®å•†å“ã¨åŒã˜ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚‚ã®ã¯é™¤å¤–
    active_titles = set(active_items['title'].unique())
    original_count = len(ended_items)
    ended_items = ended_items[~ended_items['title'].isin(active_titles)]
    if len(ended_items) < original_count:
        log(f"â„¹ï¸ ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ã«ã‚ˆã‚Š {original_count - len(ended_items)} ä»¶ã®å•†å“ã‚’å†å‡ºå“å¯¾è±¡ã‹ã‚‰é™¤å¤–ã—ã¾ã—ãŸã€‚")

    if ended_items.empty:
        log("âœ… å†å‡ºå“å¯èƒ½ãªï¼ˆæœªå‡¦ç†ã®ï¼‰çµ‚äº†å•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # å„ªå…ˆé †ä½ã§ã‚½ãƒ¼ãƒˆ
    ended_items.sort_values(by=['bids', 'watch', 'access'], ascending=False, inplace=True)
    
    items_to_relist = ended_items.head(num_to_relist)
    log(f"å†å‡ºå“å¯¾è±¡ã¨ã—ã¦ {len(items_to_relist)} ä»¶ã‚’é¸æŠã—ã¾ã—ãŸã€‚")
    if len(items_to_relist) > 0:
        print(items_to_relist[['auction_id', 'title', 'bids', 'watch', 'access']].to_string())

    for index, item in items_to_relist.iterrows():
        auction_id = item['auction_id']
        log(f"\n--- å†å‡ºå“å‡¦ç†ä¸­ ({items_to_relist.index.get_loc(index) + 1}/{len(items_to_relist)}): {auction_id} ---")
        if relist_item(driver, auction_id):
            save_processed_id(auction_id, PROCESSED_RELIST_LOG)
            log("  ...æ¬¡ã®å‡¦ç†ã¾ã§5ç§’å¾…æ©Ÿ...")
            time.sleep(5)
        else:
            log(f"  âš ï¸ {auction_id} ã®å†å‡ºå“ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¬¡ã®å•†å“ã«é€²ã¿ã¾ã™ã€‚")
            save_processed_id(auction_id, PROCESSED_RELIST_LOG) # å¤±æ•—ã—ã¦ã‚‚æ¬¡å›ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

def main():
    log("=== ãƒ¤ãƒ•ã‚ªã‚¯ ãƒã‚¤ãƒ»ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ===")
    driver = setup_driver()
    
    all_scraped_items = []
    
    # 1. å‡ºå“ä¸­
    log("\n--- [1/2] å‡ºå“ä¸­ã®å•†å“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ---")
    items_selling = scrape_url(driver, SELLING_URL, "å‡ºå“ä¸­")
    all_scraped_items.extend(items_selling)
    
    # 2. è½æœ­è€…ãªã—ï¼ˆçµ‚äº†åˆ†ï¼‰
    log("\n--- [2/2] è½æœ­è€…ãªã—ï¼ˆçµ‚äº†åˆ†ï¼‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ---")
    items_closed = scrape_url(driver, CLOSED_URL, "çµ‚äº†ï¼ˆè½æœ­è€…ãªã—ï¼‰")
    all_scraped_items.extend(items_closed)
    
    save_to_csv(all_scraped_items)

    # 3. å¿…è¦ã§ã‚ã‚Œã°å†å‡ºå“
    relist_if_needed(driver, all_scraped_items)
    
    log("\nå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã¯é–‹ã„ãŸã¾ã¾ã«ã—ã¾ã™ã€‚æ‰‹å‹•ã§é–‰ã˜ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()