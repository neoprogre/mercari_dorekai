import os
import glob
import json
import sys
import pandas as pd
import re
import time
import requests
from bs4 import BeautifulSoup

def load_brand_name_list_from_master(file_path="brand_master_sjis.csv"):
    """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€ï¼ˆå•†å“åã‹ã‚‰ã®æŠ½å‡ºç”¨ï¼‰"""
    brands = []
    try:
        df = pd.read_csv(file_path, encoding='cp932', header=None, usecols=[1])
        brands = df[1].dropna().astype(str).tolist()
        brands.sort(key=len, reverse=True)
        print(f"ğŸ“š ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ {len(brands)} ä»¶ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except FileNotFoundError:
        print(f"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚å•†å“åã‹ã‚‰ã®ãƒ–ãƒ©ãƒ³ãƒ‰æŠ½å‡ºã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
    return brands

def load_brand_master_map(file_path="brand_master_sjis.csv"):
    """ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {ãƒ–ãƒ©ãƒ³ãƒ‰ID: {å„åç§°}} ã®è¾æ›¸ã‚’ä½œæˆã™ã‚‹"""
    brand_map = {}
    try:
        # Shift_JISã§èª­ã¿è¾¼ã¿ã€ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã„ã“ã¨ã‚’æƒ³å®š
        df = pd.read_csv(file_path, encoding='cp932', header=None, dtype=str)
        # åˆ—åã‚’å®šç¾©
        df.columns = ['ãƒ–ãƒ©ãƒ³ãƒ‰ID', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰']
        df.dropna(subset=['ãƒ–ãƒ©ãƒ³ãƒ‰ID'], inplace=True)
        # ãƒ–ãƒ©ãƒ³ãƒ‰IDã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã—ã¦è¾æ›¸åŒ–
        brand_map = df.set_index('ãƒ–ãƒ©ãƒ³ãƒ‰ID').to_dict('index')
        print(f"ğŸ“š ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼è¾æ›¸ã‚’ {len(brand_map)} ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except FileNotFoundError:
        print(f"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚IDã‹ã‚‰ã®ãƒ–ãƒ©ãƒ³ãƒ‰åè§£æ±ºã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
    except Exception as e:
        print(f"âŒ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    return brand_map

def extract_product_number(name):
    """å•†å“åã‹ã‚‰å“ç•ªï¼ˆå…ˆé ­ã®3-5æ¡ã®æ•°å­—ï¼‰ã‚’æŠ½å‡ºã™ã‚‹"""
    if not isinstance(name, str):
        return None
    match = re.match(r'^(\d{3,5})\s', name)
    return match.group(1) if match else None

def add_duplicate_column(df, subset_col='å“ç•ª'):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«é‡è¤‡ãƒã‚§ãƒƒã‚¯åˆ—ã‚’è¿½åŠ ã™ã‚‹"""
    df['é‡è¤‡'] = ''
    if not df.empty and subset_col in df.columns:
        # keep=Falseã¯é‡è¤‡ã™ã‚‹ã™ã¹ã¦ã®è¡Œã‚’Trueã«ã™ã‚‹
        duplicates = df.duplicated(subset=[subset_col], keep=False) & df[subset_col].notna()
        df.loc[duplicates, 'é‡è¤‡'] = 'é‡è¤‡'
    return df

def process_rakuma_data():
    # --- [è¨­å®š] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ãƒšãƒ¼ã‚¸ ---
    # Trueã«ã™ã‚‹ã¨ã‚·ãƒ§ãƒƒãƒ—ã®å…¨ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚
    # Falseã«ã™ã‚‹ã¨1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã‚’å¯¾è±¡ã«ã—ã¾ã™ã€‚
    SCRAPE_ALL_PAGES = True
    # ------------------------------------
    # --- ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆé€†å¼•ãç”¨ï¼‰ ---
    CONDITION_MAP_INV = {'æ–°å“ã€æœªä½¿ç”¨': '1', 'æœªä½¿ç”¨ã«è¿‘ã„': '2', 'ç›®ç«‹ã£ãŸå‚·ã‚„æ±šã‚Œãªã—': '3', 'ã‚„ã‚„å‚·ã‚„æ±šã‚Œã‚ã‚Š': '4', 'å‚·ã‚„æ±šã‚Œã‚ã‚Š': '5', 'å…¨ä½“çš„ã«çŠ¶æ…‹ãŒæ‚ªã„': '6'}
    SHIPPING_PAYER_MAP_INV = {'é€æ–™è¾¼ã¿(å‡ºå“è€…è² æ‹…)': '1', 'ç€æ‰•ã„(è³¼å…¥è€…è² æ‹…)': '2', 'é€æ–™è¾¼': '1'}
    DAYS_TO_SHIP_MAP_INV = {'1-2æ—¥ã§ç™ºé€': '1', '2-3æ—¥ã§ç™ºé€': '2', '4-7æ—¥ã§ç™ºé€': '3', 'æ”¯æ‰•ã„å¾Œã€1ï½2æ—¥ã§ç™ºé€': '1', 'æ”¯æ‰•ã„å¾Œã€2ï½3æ—¥ã§ç™ºé€': '2', 'æ”¯æ‰•ã„å¾Œã€4ï½7æ—¥ã§ç™ºé€': '3'}
    # PREFECTURE_MAPã¯æ•°ãŒå¤šã„ãŸã‚ã€å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 

    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒ©ã‚¯ãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ã—ã¦æ•´å½¢ã™ã‚‹"""
    print("Processing Rakuma data from web...")
    base_url = 'https://fril.jp/shop/3c65d78bc0e1eadbe2a3528b344d8311'
    page = 1
    all_products = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    product_links = []
    while True:
        url = f"{base_url}?page={page}"
        print(f"Scraping Rakuma page: {url}")
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
        except requests.exceptions.RequestException as e:
            print(f"Error fetching Rakuma page: {e}")
            break

        soup = BeautifulSoup(response.content, 'html.parser')
        items = soup.find_all('div', class_='item', attrs={'data-test': 'item'})

        if not items:
            print(f"No more items found on page {page}. Stopping.")
            break

        for item in items:            
            link_tag = item.find('a', class_='link_shop_image')
            if link_tag and 'href' in link_tag.attrs:
                product_links.append(link_tag['href'])
        
        # 1ãƒšãƒ¼ã‚¸ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹å ´åˆã€ã“ã“ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        if not SCRAPE_ALL_PAGES:
            print("1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
            break

        page += 1
        time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã®å¾…æ©Ÿ

    print(f"Found {len(product_links)} product links. Now scraping details for each product...")

    for item_url in product_links:
        print(f"  Scraping details from: {item_url}")
        try:
            response = requests.get(item_url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # åŸºæœ¬æƒ…å ±ã®å–å¾—
            name = soup.find('h1', class_='item__name').text.strip() if soup.find('h1', class_='item__name') else 'N/A'
            # [å†ä¿®æ­£] ä¾¡æ ¼å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã•ã‚‰ã«å¼·åŒ–
            price = 'N/A'
            price_selectors = [
                'span[data-price]', # å±æ€§ã‹ã‚‰ç›´æ¥å–å¾— (æœ€ã‚‚ç¢ºå®Ÿ)
                'p.item__price',    # ä»¥å‰ã®æ§‹é€ 
                'div.item__price',  # divã‚¿ã‚°ã®å¯èƒ½æ€§
                'span.item__price', # spanã‚¿ã‚°ã®å¯èƒ½æ€§
            ]
            for selector in price_selectors:
                price_tag = soup.select_one(selector)
                if price_tag:
                    if 'data-price' in price_tag.attrs:
                        price = price_tag['data-price']
                        break
                    price_text = price_tag.get_text(strip=True)
                    price_match = re.search(r'[\d,]+', price_text)
                    if price_match:
                        price = price_match.group(0).replace(',', '')
                        break

            is_sold_out = soup.find('div', class_='item-box__soldout_ribbon') is not None

            # å•†å“èª¬æ˜ã®å–å¾—
            description = 'N/A'
            desc_tag = soup.find('div', class_='item__description__line-limited')
            if desc_tag:
                # <br>ã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                for br in desc_tag.find_all("br"):
                    br.replace_with("\n")
                description = desc_tag.text.strip()

            # å•†å“æƒ…å ±ã®å–å¾—
            product_info = {}
            details_table = soup.find('table', class_='item__details')
            if details_table:
                for row in details_table.find_all('tr'):
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.text.strip()
                        # ã‚«ãƒ†ã‚´ãƒªã¯ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                        if key == 'ã‚«ãƒ†ã‚´ãƒª':
                            value = ' > '.join([a.text.strip() for a in td.find_all('a')])
                        # ãƒ–ãƒ©ãƒ³ãƒ‰åã¯<a>ã‚¿ã‚°ã®ä¸­ã«ã‚ã‚‹å ´åˆã¨ã€tdç›´ä¸‹ã«ã‚ã‚‹å ´åˆã®ä¸¡æ–¹ã«å¯¾å¿œ
                        elif key == 'ãƒ–ãƒ©ãƒ³ãƒ‰':
                            brand_link = td.find('a')
                            if brand_link:
                                value = brand_link.text.strip()
                            else:
                                value = td.text.strip()
                        else:
                            value = td.text.strip()
                        product_info[key] = value
            
            # å–å¾—ã—ãŸæƒ…å ±ã‚’ã¾ã¨ã‚ã‚‹
            product_data = {
                'å•†å“å': name,
                'ä¾¡æ ¼': price,
                'URL': item_url,
                'is_sold_out': is_sold_out,
                'å•†å“èª¬æ˜': description,
                'ãƒ–ãƒ©ãƒ³ãƒ‰': product_info.get('ãƒ–ãƒ©ãƒ³ãƒ‰', ''), # [ä¿®æ­£] ãƒ©ã‚¯ãƒãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã—ãŸãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿ã‚’è¨­å®š
                'ã‚«ãƒ†ã‚´ãƒª': product_info.get('ã‚«ãƒ†ã‚´ãƒª', ''),
                'ã‚µã‚¤ã‚º': product_info.get('ã‚µã‚¤ã‚º', ''),
                'å•†å“ã®çŠ¶æ…‹': product_info.get('å•†å“ã®çŠ¶æ…‹', ''),
                'é…é€æ–™ã®è² æ‹…': product_info.get('é…é€æ–™ã®è² æ‹…', ''),
                'é…é€æ–¹æ³•': product_info.get('é…é€æ–¹æ³•', ''),
                'ç™ºé€æ—¥ã®ç›®å®‰': product_info.get('ç™ºé€æ—¥ã®ç›®å®‰', ''),
                'ç™ºé€å…ƒã®åœ°åŸŸ': product_info.get('ç™ºé€å…ƒã®åœ°åŸŸ', ''),
            }
            all_products.append(product_data)

        except requests.exceptions.RequestException as e:
            print(f"  Error fetching item page {item_url}: {e}", file=sys.stderr)
        except Exception as e:
            print(f"  Error parsing item page {item_url}: {e}", file=sys.stderr)
        
        time.sleep(1)

    if not all_products:
        print("No products found in Rakuma data.")
        return pd.DataFrame()

    df = pd.DataFrame(all_products)
    df['å“ç•ª'] = df['å•†å“å'].apply(extract_product_number)

    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
    df['å•†å“ã®çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰'] = df['å•†å“ã®çŠ¶æ…‹'].map(CONDITION_MAP_INV)
    df['é…é€æ–™è² æ‹…ã‚³ãƒ¼ãƒ‰'] = df['é…é€æ–™ã®è² æ‹…'].map(SHIPPING_PAYER_MAP_INV)
    df['ç™ºé€æ—¥ã®ç›®å®‰ã‚³ãƒ¼ãƒ‰'] = df['ç™ºé€æ—¥ã®ç›®å®‰'].map(DAYS_TO_SHIP_MAP_INV)

    df = add_duplicate_column(df)
    
    # å‡ºåŠ›ã™ã‚‹åˆ—ã‚’å®šç¾©
    output_cols = [
        'å“ç•ª', 'é‡è¤‡', 'å•†å“å', 'ä¾¡æ ¼', 'URL', 'is_sold_out', 'å•†å“èª¬æ˜', 'ãƒ–ãƒ©ãƒ³ãƒ‰',
        'ã‚«ãƒ†ã‚´ãƒª', 'ã‚µã‚¤ã‚º', 'å•†å“ã®çŠ¶æ…‹', 'é…é€æ–™ã®è² æ‹…', 'é…é€æ–¹æ³•',
        'ç™ºé€æ—¥ã®ç›®å®‰', 'ç™ºé€å…ƒã®åœ°åŸŸ', 'å•†å“ã®çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰', 'é…é€æ–™è² æ‹…ã‚³ãƒ¼ãƒ‰', 'ç™ºé€æ—¥ã®ç›®å®‰ã‚³ãƒ¼ãƒ‰'
    ]
    # å­˜åœ¨ã—ãªã„åˆ—ãŒã‚ã‚Œã°ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
    for col in output_cols:
        if col not in df.columns:
            df[col] = ''

    df = df[output_cols]
    print(f"Found {len(df)} products in Rakuma data.")
    return df

def process_mercari_data():
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã®æœ€æ–°ã®Mercari CSVã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€æ•´å½¢ã™ã‚‹"""
    print("Processing Mercari data...")
    try:
        mercari_path = r'\\LS210DNBD82\share\å¹³è‰¯\Python\mercari_dorekai'
        search_pattern = os.path.join(mercari_path, 'product_data_*.csv') # [ä¿®æ­£] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«åˆã‚ã›ã¦ãƒ‘ã‚¹ã‚’ä¿®æ­£
        
        files = glob.glob(search_pattern)
        if not files:
            print(f"No Mercari CSV files found at: {search_pattern}")
            return pd.DataFrame(), {} # [ä¿®æ­£] ç©ºã®è¾æ›¸ã‚‚è¿”ã™
        
        latest_file = max(files, key=os.path.getmtime)
        print(f"Processing latest Mercari file: {latest_file}")
        
        df = pd.read_csv(latest_file, encoding='cp932')
        
        df = df.rename(columns={'è²©å£²ä¾¡æ ¼': 'ä¾¡æ ¼'})
        
        if 'å•†å“ID' in df.columns:
            df['URL'] = 'https://jp.mercari.com/shops/product/' + df['å•†å“ID'].astype(str)
        else:
            print("Warning: 'å•†å“ID' column not found in Mercari CSV. URL will be empty.")
            df['URL'] = ''

        df['å“ç•ª'] = df['å•†å“å'].apply(extract_product_number)
        df = add_duplicate_column(df)

        # å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        if 'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹' not in df.columns:
            df['å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'] = '0'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦ '0' (è²©å£²ä¸­) ã‚’è¨­å®š
        
        # [è¿½åŠ ] ãƒ–ãƒ©ãƒ³ãƒ‰IDã¨å“ç•ªã®ãƒãƒƒãƒ—ã‚’ä½œæˆ
        hinban_to_brandid_map = {}
        if 'å“ç•ª' in df.columns and 'ãƒ–ãƒ©ãƒ³ãƒ‰ID' in df.columns:
            df_map = df[['å“ç•ª', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID']].copy()
            df_map.dropna(subset=['å“ç•ª', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID'], inplace=True)
            df_map['å“ç•ª'] = df_map['å“ç•ª'].astype(str)
            hinban_to_brandid_map = pd.Series(df_map['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].values, index=df_map['å“ç•ª']).to_dict()
            print(f"ğŸ“š å“ç•ª->ãƒ–ãƒ©ãƒ³ãƒ‰IDè¾æ›¸ã‚’ {len(hinban_to_brandid_map)} ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

        final_cols = ['å“ç•ª', 'é‡è¤‡', 'å•†å“å', 'ä¾¡æ ¼', 'URL', 'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'å•†å“ID', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID']
        for col in final_cols:
            if col not in df.columns:
                df[col] = ''
        
        print(f"Found {len(df)} products in Mercari data.")
        return df[final_cols].copy(), hinban_to_brandid_map

    except Exception as e:
        print(f"An error occurred while processing Mercari data: {e}")
        return pd.DataFrame(), {}

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    rakuma_df = process_rakuma_data()
    mercari_df, hinban_to_brandid_map = process_mercari_data()
    brand_master_map = load_brand_master_map()

    # --- [æ–°è¦] ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰å¼•ã„ãŸãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¨­å®š ---
    if not rakuma_df.empty and hinban_to_brandid_map and brand_master_map:
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'] = rakuma_df['å“ç•ª'].astype(str).map(hinban_to_brandid_map)

        # ãƒ–ãƒ©ãƒ³ãƒ‰IDã«åŸºã¥ã„ã¦å„ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¨­å®šã™ã‚‹é–¢æ•°
        def get_brand_details(brand_id, column_name):
            if pd.isna(brand_id):
                return None
            return brand_master_map.get(str(brand_id), {}).get(column_name, None)

        # æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰å'] = rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].apply(lambda x: get_brand_details(x, 'ãƒ–ãƒ©ãƒ³ãƒ‰å'))
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰'] = rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].apply(lambda x: get_brand_details(x, 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰'))
        rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰'] = rakuma_df['ãƒ–ãƒ©ãƒ³ãƒ‰ID'].apply(lambda x: get_brand_details(x, 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰'))
        # [å‰Šé™¤] æ—¢å­˜ã®ãƒ–ãƒ©ãƒ³ãƒ‰åˆ—ã®æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤

    # --- ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«ãƒ¡ãƒ«ã‚«ãƒªã®å•†å“IDã‚’ç´ä»˜ã‘ã‚‹å‡¦ç†ã‚’è¿½åŠ  ---
    print("ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«ãƒ¡ãƒ«ã‚«ãƒªã®å•†å“IDã‚’ç´ä»˜ã‘ã¾ã™...")
    if not mercari_df.empty and 'å“ç•ª' in mercari_df.columns and 'å•†å“ID' in mercari_df.columns:
        # ãƒ¡ãƒ«ã‚«ãƒªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å“ç•ªã¨å•†å“IDã®ã¿ã‚’æŠ½å‡ºï¼ˆé‡è¤‡ã¯æœ€åˆã®ã‚‚ã®ã‚’æ¡ç”¨ï¼‰
        mercari_id_map = mercari_df.drop_duplicates(subset=['å“ç•ª'])[['å“ç•ª', 'å•†å“ID']].copy()
        # å“ç•ªã‚’æ–‡å­—åˆ—ã«çµ±ä¸€ã—ã¦ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
        mercari_id_map['å“ç•ª'] = mercari_id_map['å“ç•ª'].astype(str)
        rakuma_df['å“ç•ª'] = rakuma_df['å“ç•ª'].astype(str)

        # ãƒ©ã‚¯ãƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ¡ãƒ«ã‚«ãƒªã®æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
        rakuma_df = pd.merge(rakuma_df, mercari_id_map, on='å“ç•ª', how='left')
        # ãƒãƒ¼ã‚¸ã«ã‚ˆã£ã¦ 'å•†å“ID_x', 'å•†å“ID_y' ãŒã§ãã‚‹ã®ã‚’é˜²ããŸã‚ã€å…ƒã® 'å•†å“ID' ã‚’å„ªå…ˆ
        rakuma_df.rename(columns={'å•†å“ID_x': 'å•†å“ID'}, inplace=True)
        print("ãƒ¡ãƒ«ã‚«ãƒªå•†å“IDã®ç´ä»˜ã‘ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    # ---------------------------------------------------------
    
    print("ãƒ©ã‚¯ãƒãƒ‡ãƒ¼ã‚¿ã«å‰Šé™¤åˆ—ã‚’è¿½åŠ ã™ã‚‹å‡¦ç†ã‚’é–‹å§‹...")
    
    # Mercariã®å“ç•ªã‚’ã‚­ãƒ¼ã€å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
    if 'å“ç•ª' in mercari_df.columns and 'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹' in mercari_df.columns:
        # NaNã‚’è€ƒæ…®ã—ã€dropna()ã‚’è¿½åŠ ã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¯æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
        mercari_status_map = pd.Series(mercari_df['å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'].astype(str).values, index=mercari_df['å“ç•ª']).dropna().to_dict()
    else:
        print("âš ï¸ Mercariãƒ‡ãƒ¼ã‚¿ã«'å“ç•ª'ã¾ãŸã¯'å•†å“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'åˆ—ãŒãªã„ãŸã‚ã€å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        mercari_status_map = {}

    def get_delete_status(row):
        # ãƒ©ã‚¯ãƒã§SOLD OUTã®å ´åˆã¯å‰Šé™¤ã—ãªã„
        if row.get('is_sold_out', False):
            return ''

        hinban = row['å“ç•ª']
        # å“ç•ªãŒNaNã‚„Noneã®å ´åˆã¯ãƒã‚§ãƒƒã‚¯ã—ãªã„
        if pd.isna(hinban):
            return ''
        
        mercari_status = mercari_status_map.get(str(hinban))
        
        if mercari_status is None: # æ¡ä»¶1: Mercariã«å“ç•ªãŒå­˜åœ¨ã—ãªã„
            return 'å‰Šé™¤'

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ '1' (å£²åˆ‡ã‚Œ) ã®å ´åˆ
        if str(mercari_status) == '1': # æ¡ä»¶2: Mercariã§ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ'1'
            return 'å‰Šé™¤'
            
        return ''

    # 'å“ç•ª'åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‰Šé™¤åˆ—ã‚’è¿½åŠ 
    if 'å“ç•ª' in rakuma_df.columns:
        # is_sold_outåˆ—ã‚’å…ˆã«å‡¦ç†
        if 'is_sold_out' not in rakuma_df.columns:
            rakuma_df['is_sold_out'] = False
        else:
            rakuma_df['is_sold_out'] = rakuma_df['is_sold_out'].fillna(False)

        rakuma_df['å‰Šé™¤'] = rakuma_df.apply(get_delete_status, axis=1)
        # is_sold_outåˆ—ã‚’å‰Šé™¤
        if 'is_sold_out' in rakuma_df.columns:
            rakuma_df = rakuma_df.drop(columns=['is_sold_out'])

        # --- åˆ—ã®é †åºã‚’æœ€çµ‚èª¿æ•´ ---
        # åŸºæœ¬ã¨ãªã‚‹åˆ—ã®é †åºã‚’å®šç¾©
        final_cols_order = [
            'å“ç•ª', 'é‡è¤‡', 'å•†å“ID', 'å‰Šé™¤', 'å•†å“å', 'ä¾¡æ ¼', 'URL', 'å•†å“èª¬æ˜', 'ãƒ–ãƒ©ãƒ³ãƒ‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆã‚«ãƒŠï¼‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè‹±èªï¼‰',
            'ã‚«ãƒ†ã‚´ãƒª', 'ã‚µã‚¤ã‚º', 'å•†å“ã®çŠ¶æ…‹', 'é…é€æ–™ã®è² æ‹…', 'é…é€æ–¹æ³•', 'ãƒ–ãƒ©ãƒ³ãƒ‰ID',
            'ç™ºé€æ—¥ã®ç›®å®‰', 'ç™ºé€å…ƒã®åœ°åŸŸ', 'å•†å“ã®çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰', 'é…é€æ–™è² æ‹…ã‚³ãƒ¼ãƒ‰', 'ç™ºé€æ—¥ã®ç›®å®‰ã‚³ãƒ¼ãƒ‰'
        ]
        
        # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã§é †åºã‚’å†æ§‹ç¯‰
        current_cols = rakuma_df.columns.tolist()
        ordered_cols = [col for col in final_cols_order if col in current_cols]
        
        # é †åºå®šç¾©ã«å«ã¾ã‚Œãªã„ãŒã€ä¸‡ãŒä¸€å­˜åœ¨ã™ã‚‹åˆ—ãŒã‚ã‚Œã°æœ«å°¾ã«è¿½åŠ 
        ordered_cols.extend([col for col in current_cols if col not in ordered_cols])
        
        rakuma_df = rakuma_df[ordered_cols]
    else:
        rakuma_df['å‰Šé™¤'] = ''
        print("âš ï¸ Rakumaãƒ‡ãƒ¼ã‚¿ã«'å“ç•ª'åˆ—ãŒãªã„ãŸã‚ã€å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    print("å‰Šé™¤åˆ—ã®å‡¦ç†å®Œäº†ã€‚")

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç½®ã‹ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
    script_dir = os.path.dirname(os.path.abspath(__file__))

    output_rakuma_file = os.path.join(script_dir, 'products_rakuma.csv')
    output_mercari_file = os.path.join(script_dir, 'products_mercari.csv')
    
    print(f"Writing Rakuma data to '{output_rakuma_file}'...")
    rakuma_df.to_csv(output_rakuma_file, index=False, encoding='utf-8-sig')
    
    print(f"Writing Mercari data to '{output_mercari_file}'...")
    mercari_df.to_csv(output_mercari_file, index=False, encoding='utf-8-sig')
        
    print("Script finished successfully.")

if __name__ == '__main__':
    main()
